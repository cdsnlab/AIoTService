{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# 1. Connect to Google Drive"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from google.colab import drive\n",
    "\n",
    "drive.mount('/gdrive')\n",
    "gdrive_root = '/gdrive/My Drive'"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 2. Import modules"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import MNIST\n",
    "\n",
    "!pip install pyngrok\n",
    "# import TensorBoardColab\n",
    "!pip install -U tensorboardcolab\n",
    "from tensorboardcolab import TensorBoardColab\n",
    "\n",
    "torch.manual_seed(470)\n",
    "torch.cuda.manual_seed(470)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 3. Configure the experiments"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# training & optimization hyper-parameters\n",
    "max_epoch = 10\n",
    "learning_rate = 0.0001\n",
    "batch_size = 200\n",
    "device = 'cuda'\n",
    "\n",
    "# model hyper-parameters\n",
    "input_dim = 784 # 28x28=784\n",
    "hidden_dim = 512\n",
    "output_dim = 10 \n",
    "\n",
    "# initialize tensorboard for visualization\n",
    "# Note : click the Tensorboard link to see the visualization of training/testing results\n",
    "# tbc = TensorBoardColab()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 4. Construct data pipeline"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "data_dir = os.path.join(gdrive_root, 'my_data')\n",
    "\n",
    "transform = transforms.ToTensor()\n",
    "\n",
    "train_dataset = MNIST(data_dir, train=True, download=True, transform=transform)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "\n",
    "test_dataset = MNIST(data_dir, train=False, download=True, transform=transform)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, drop_last=False)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 5. Construct a neural network builder"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "class MyClassifier(nn.Module):\n",
    "  def __init__(self, input_dim=784, hidden_dim=512, output_dim=10):\n",
    "    super(MyClassifier, self).__init__()\n",
    "    self.layers = nn.Sequential(\n",
    "      nn.Linear(input_dim, hidden_dim),\n",
    "      nn.ReLU(),\n",
    "      nn.Linear(hidden_dim, hidden_dim),\n",
    "      nn.ReLU(),\n",
    "      nn.Linear(hidden_dim, hidden_dim),\n",
    "      nn.ReLU(),\n",
    "      nn.Linear(hidden_dim, output_dim),\n",
    "    )\n",
    "    \n",
    "  def forward(self, x):\n",
    "    batch_size = x.size(0)\n",
    "    x = x.view(batch_size, -1)\n",
    "    outputs = self.layers(x)\n",
    "    return outputs"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 6. Initialize the model and optimizer"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "my_classifier = MyClassifier(input_dim, hidden_dim, output_dim)\n",
    "my_classifier = my_classifier.to(device)\n",
    "\n",
    "optimizer = optim.Adam(my_classifier.parameters(), lr=learning_rate)\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 7. Load pre-trained weights if exist"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "ckpt_dir = os.path.join(gdrive_root, 'checkpoints')\n",
    "if not os.path.exists(ckpt_dir):\n",
    "  os.makedirs(ckpt_dir)\n",
    "  \n",
    "best_acc = 0.\n",
    "ckpt_path = os.path.join(ckpt_dir, 'lastest.pt')\n",
    "if os.path.exists(ckpt_path):\n",
    "  ckpt = torch.load(ckpt_path)\n",
    "  try:\n",
    "    my_classifier.load_state_dict(ckpt['my_classifier'])\n",
    "    optimizer.load_state_dict(ckpt['optimizer'])\n",
    "    best_acc = ckpt['best_acc']\n",
    "  except RuntimeError as e:\n",
    "      print('wrong checkpoint')\n",
    "  else:    \n",
    "    print('checkpoint is loaded !')\n",
    "    print('current best accuracy : %.2f' % best_acc)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 8. Train"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "it = 0\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "for epoch in range(max_epoch):\n",
    "  # train phase\n",
    "  # Note: Behaviours of some layers/modules, such as dropout, batchnorm, etc., are different (or should be treated differently) depending on whether the phase is in train or test\n",
    "  #       For example, dropout modules turn off some activations with probability p in training time, but not in test time.\n",
    "  #       However, our network \"my_classifier\" does not know which phase is under-going, and we need to give the network a signal to handle this issue.\n",
    "  #       Fortuntely, Pytorch provides us the utility functions for this, which are `.train()` and `.eval()`\n",
    "  my_classifier.train()\n",
    "  for inputs, labels in train_dataloader:\n",
    "    it += 1\n",
    "    \n",
    "    # load data to the GPU.\n",
    "    inputs = inputs.to(device)\n",
    "    labels = labels.to(device)\n",
    "    \n",
    "    # feed data into the network and get outputs.\n",
    "    logits = my_classifier(inputs)\n",
    "    \n",
    "    # calculate loss\n",
    "    # Note: `F.cross_entropy` function receives logits, or pre-softmax outputs, rather than final probability scores.\n",
    "    loss = F.cross_entropy(logits, labels)\n",
    "    \n",
    "    # Note: You should flush out gradients computed at the previous step before computing gradients at the current step. \n",
    "    #       Otherwise, gradients will accumulate.\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # backprogate loss.\n",
    "    loss.backward()\n",
    "    \n",
    "    # update the weights in the network.\n",
    "    optimizer.step()\n",
    "    \n",
    "    # calculate accuracy.\n",
    "    acc = (logits.argmax(dim=1) == labels).float().mean()\n",
    "    \n",
    "    if it % 200 == 0:\n",
    "      # tbc.save_value('Loss', 'train_loss', it, loss.item())\n",
    "      print('[epoch:{}, iteration:{}] train loss : {:.4f} train accuracy : {:.4f}'.format(epoch, it, loss.item(), acc.item()))\n",
    "    \n",
    "  # save losses in a list so that we can visualize them later.\n",
    "  train_losses.append(loss.item())  \n",
    "    \n",
    "  # test phase\n",
    "  n = 0.\n",
    "  test_loss = 0.\n",
    "  test_acc = 0.\n",
    "  my_classifier.eval()\n",
    "  for test_inputs, test_labels in test_dataloader:\n",
    "    test_inputs = test_inputs.to(device)\n",
    "    test_labels = test_labels.to(device)\n",
    "    \n",
    "    logits = my_classifier(test_inputs)\n",
    "    test_loss += F.cross_entropy(logits, test_labels, reduction='sum')\n",
    "    test_acc += (logits.argmax(dim=1) == test_labels).float().sum()\n",
    "    n += test_inputs.size(0)\n",
    "    \n",
    "  test_loss /= n\n",
    "  test_acc /= n\n",
    "  test_losses.append(test_loss.item())\n",
    "  # tbc.save_value('Loss', 'test_loss', it, test_loss.item())\n",
    "  print('[epoch:{}, iteration:{}] test_loss : {:.4f} test accuracy : {:.4f}'.format(epoch, it, test_loss.item(), test_acc.item())) \n",
    "  \n",
    "  # tbc.flush_line('train_loss')\n",
    "  # tbc.flush_line('test_loss')\n",
    "  \n",
    "  # save checkpoint whenever there is improvement in performance\n",
    "  if test_acc > best_acc:\n",
    "    best_acc = test_acc\n",
    "    # Note: optimizer also has states ! don't forget to save them as well.\n",
    "    ckpt = {'my_classifier':my_classifier.state_dict(),\n",
    "            'optimizer':optimizer.state_dict(),\n",
    "            'best_acc':best_acc}\n",
    "    torch.save(ckpt, ckpt_path)\n",
    "    print('checkpoint is saved !')\n",
    "    \n",
    "# tbc.close()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 9. Visualize results"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(train_losses, label='train loss')\n",
    "plt.plot(test_losses, label='test loss')\n",
    "plt.legend()"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}