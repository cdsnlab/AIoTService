{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# 1. Import"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "print(torch.__version__)\n",
    "from torch import cuda\n",
    "print(cuda.get_device_name(cuda.current_device()))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 2. Load data"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "EPOCHS = 30\n",
    "\n",
    "train_kwargs = {'batch_size': 128, 'shuffle': True}\n",
    "valid_kwargs = {'batch_size': 128, 'shuffle': False}\n",
    "test_kwargs = {'batch_size': 128, 'shuffle': False}\n",
    "\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "transform=transforms.Compose([\n",
    "        # Pad images with 0s\n",
    "        transforms.Pad((0,4,4,0), fill=0, padding_mode='constant'),\n",
    "    \n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.1307,), (0.3081,)),\n",
    "        ])\n",
    "dataset_full = datasets.MNIST('../data', train=True, download=True,\n",
    "                   transform=transform)\n",
    "valid_size = 5000\n",
    "train_size = len(dataset_full) - 5000\n",
    "dataset_train, dataset_valid = torch.utils.data.random_split(dataset_full, [train_size, valid_size])\n",
    "\n",
    "dataset_test = datasets.MNIST('../data', train=False,\n",
    "                   transform=transform)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset_train,**train_kwargs)\n",
    "valid_loader = torch.utils.data.DataLoader(dataset_valid,**valid_kwargs)\n",
    "test_loader = torch.utils.data.DataLoader(dataset_test, **test_kwargs)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print()\n",
    "print(\"Image Shape: {}\".format(dataset_train[0][0].shape))\n",
    "print()\n",
    "print(\"Training Set:   {} samples\".format(len(dataset_train)))\n",
    "print(\"Validation Set:   {} samples\".format(len(dataset_valid)))\n",
    "print(\"Test Set:       {} samples\".format(len(dataset_test)))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import pdb\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "index = random.randint(0, len(dataset_train))\n",
    "image = dataset_train[index][0].squeeze()\n",
    "\n",
    "plt.figure(figsize=(2,2))\n",
    "plt.imshow(image)\n",
    "print(\"Label of the image is:%d\"%dataset_train[index][1])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 3. LeNet"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Pytorch modification From https://www.kaggle.com/usingtc/lenet-with-pytorch\n",
    "\n",
    "from torch import nn, optim\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class LeNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LeNet, self).__init__()\n",
    "        # Layer 1: Convolutional. Input = 32x32x1. Output = 28x28x6.\n",
    "        self.conv1 = nn.Conv2d(1, 6, (5,5))\n",
    "        # Layer 2: Convolutional. Output = 10x10x16.\n",
    "        self.conv2 = nn.Conv2d(6, 16, (5,5))\n",
    "        # Layer 3: Fully Connected. Input = 400. Output = 120.\n",
    "        self.fc1   = nn.Linear(400, 120)\n",
    "        # Layer 4: Fully Connected. Input = 120. Output = 84.\n",
    "        self.fc2   = nn.Linear(120, 84)\n",
    "        # Layer 5: Fully Connected. Input = 84. Output = 10.\n",
    "        self.fc3   = nn.Linear(84, 10)\n",
    "    def forward(self, x):\n",
    "        # Activation. # Pooling. Input = 28x28x6. Output = 14x14x6.\n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)), (2,2))\n",
    "         # Activation. # Pooling. Input = 10x10x16. Output = 5x5x16.\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)), (2,2))\n",
    "        # Flatten. Input = 5x5x16. Output = 400.\n",
    "        x = x.flatten(start_dim=1)\n",
    "        # Activation.\n",
    "        x = F.relu(self.fc1(x))\n",
    "        # Activation.\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "    def num_flat_features(self, x):\n",
    "        size = x.size()[1:]\n",
    "        num_features = 1\n",
    "        for s in size:\n",
    "            num_features *= s\n",
    "        return num_features "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 4. initialize network and optimizer"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Train & test part from https://github.com/activatedgeek/LeNet-5\n",
    "def train(epoch):\n",
    "    global cur_batch_win\n",
    "    net.train()\n",
    "    loss_list, batch_list = [], []\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        output = net(images)\n",
    "        loss = criterion(output, labels)\n",
    "\n",
    "        loss_list.append(loss.detach().cpu().item())\n",
    "        batch_list.append(i+1)\n",
    "\n",
    "        #if i % 10 == 0:\n",
    "        #    print('Train - Epoch %d, Batch: %d, Loss: %f' % (epoch, i, loss.detach().cpu().item()))\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "net = LeNet()\n",
    "print (net)\n",
    "\n",
    "learning_rate = 0.001\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "#optimizer = optim.Adam(net.parameters(), lr=learning_rate)\n",
    "optimizer = optim.SGD(net.parameters(), lr=learning_rate)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(\"Training...\")\n",
    "print()\n",
    "for e in range(1, EPOCHS):\n",
    "    train(e)\n",
    "    validation_accuracy, validation_predictions = evaluate(valid_loader, dataset_valid)\n",
    "    print(\"EPOCH {} ...\".format(e))\n",
    "    print(\"Validation Accuracy = {:.3f}\".format(validation_accuracy))\n",
    "    print()\n",
    "\n",
    "torch.save(\n",
    "    {\n",
    "        'lenet': net.state_dict(),\n",
    "        'opt': optimizer.state_dict(),\n",
    "    },\n",
    "    ('model_saved.model'),\n",
    ")\n",
    "print(\"Model saved\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 5. Evaluate"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def evaluate(target_loader, target_dataset):\n",
    "    predictions = []\n",
    "    net.eval()\n",
    "    total_correct = 0\n",
    "    avg_loss = 0.0\n",
    "    for i, (images, labels) in enumerate(target_loader):\n",
    "        output = net(images)\n",
    "        avg_loss += criterion(output, labels).sum()\n",
    "        pred = output.detach().max(1)[1]\n",
    "        total_correct += pred.eq(labels.view_as(pred)).sum()\n",
    "        predictions.append(pred)\n",
    "\n",
    "    avg_loss /= len(target_dataset)\n",
    "    #print('Test Avg. Loss: %f, Accuracy: %f' % (avg_loss.detach().cpu().item(), float(total_correct) / len(data_test)))\n",
    "    accuracy    = float(total_correct) / len(target_dataset)\n",
    "    return accuracy, np.array(torch.cat(predictions))\n",
    "    #or if you are in latest Pytorch world\n",
    "    #return accuracy, np.array(torch.vstack(predictions))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "test_accuracy, test_predictions = evaluate(test_loader, dataset_test)\n",
    "print(\"Test Accuracy = {:.3f}\".format(test_accuracy))"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}