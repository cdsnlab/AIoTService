{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import tensorflow as tf\r\n",
    "tf.debugging.set_log_device_placement(True)\r\n",
    "import keras as keras\r\n",
    "from keras.models import Sequential\r\n",
    "from keras.layers import LSTM, Dense, Dropout, Activation\r\n",
    "from keras import optimizers\r\n",
    "from keras.utils import to_categorical\r\n",
    "import numpy as np\r\n",
    "import pandas as pd\r\n",
    "from sklearn.model_selection import train_test_split\r\n",
    "from random import randint\r\n",
    "from numpy import array\r\n",
    "from numpy import argmax\r\n",
    "from pandas import DataFrame\r\n",
    "from pandas import concat\r\n",
    "from keras.layers import GRU, LSTM, Activation, Bidirectional\r\n",
    "from keras.layers import Dense, Embedding\r\n",
    "from keras.layers import TimeDistributed\r\n",
    "from keras.layers import Concatenate, Flatten \r\n",
    "from keras.layers import Dense, concatenate\r\n",
    "import time\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "# stacked generalization with linear meta model on blobs dataset\r\n",
    "from sklearn.datasets import make_blobs\r\n",
    "from sklearn.metrics import accuracy_score\r\n",
    "from sklearn.linear_model import LogisticRegression\r\n",
    "from keras.models import load_model\r\n",
    "from numpy import dstack\r\n",
    "#from attention import Attention\r\n",
    "\r\n",
    "# univariate cnn lstm example\r\n",
    "from numpy import array\r\n",
    "from keras.models import Sequential\r\n",
    "from keras.layers import LSTM\r\n",
    "from keras.layers import Dense\r\n",
    "from keras.layers import Flatten\r\n",
    "from keras.layers import TimeDistributed\r\n",
    "from keras.layers.convolutional import Conv1D\r\n",
    "from keras.layers.convolutional import MaxPooling1D\r\n",
    "from keras.layers import SpatialDropout1D\r\n",
    "from keras.layers import ConvLSTM2D\r\n",
    "from scipy.optimize import differential_evolution\r\n",
    "from sklearn.datasets import make_blobs\r\n",
    "\r\n",
    "from sklearn.metrics import accuracy_score\r\n",
    "from sklearn.metrics import precision_score\r\n",
    "from sklearn.metrics import recall_score\r\n",
    "from sklearn.metrics import f1_score\r\n",
    "from sklearn.metrics import roc_auc_score\r\n",
    "from sklearn.metrics import cohen_kappa_score\r\n",
    "from sklearn.metrics import confusion_matrix\r\n",
    "\r\n",
    "from keras.utils import to_categorical\r\n",
    "from keras.models import Sequential\r\n",
    "from keras.layers import Dense\r\n",
    "from matplotlib import pyplot\r\n",
    "from numpy import mean\r\n",
    "from numpy import std\r\n",
    "from numpy import array\r\n",
    "from numpy import argmax\r\n",
    "from numpy import tensordot\r\n",
    "from numpy.linalg import norm\r\n",
    "from itertools import product\r\n",
    "from keras.layers import Dense, Embedding, Bidirectional, LSTM, Concatenate, Dropout\r\n",
    "from keras import Input, Model\r\n",
    "from keras import optimizers\r\n",
    "import os\r\n",
    "import tensorflow as tf\r\n",
    "from tensorflow import keras\r\n",
    "from tensorflow.keras import layers\r\n",
    "#For Tokenizer\r\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\r\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "import csv"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "tf.debugging.set_log_device_placement(True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Predeined Parameters and Blocks"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "vocab_size = 1000\r\n",
    "embed_dim =8 # Embedding size for each token\r\n",
    "maxlen = 50\r\n",
    "num_heads = 2  # Number of attention heads\r\n",
    "ff_dim = 64  # Hidden layer size in feed forward network inside transformer\r\n",
    "trunc_type = 'post'\r\n",
    "padding_type ='post'\r\n",
    "oov_tok = \"<OOV>\"\r\n",
    "\r\n",
    "num_epochs = 100"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "class TransformerBlock(layers.Layer):\r\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\r\n",
    "        super(TransformerBlock, self).__init__()\r\n",
    "        self.att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\r\n",
    "        self.ffn = keras.Sequential(\r\n",
    "            [layers.Dense(ff_dim, activation=\"relu\"), layers.Dense(embed_dim),]\r\n",
    "        )\r\n",
    "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\r\n",
    "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\r\n",
    "        self.dropout1 = layers.Dropout(rate)\r\n",
    "        self.dropout2 = layers.Dropout(rate)\r\n",
    "        \r\n",
    "    def call(self, inputs, training):\r\n",
    "        attn_output = self.att(inputs, inputs)\r\n",
    "        attn_output = self.dropout1(attn_output, training=training)\r\n",
    "        out1 = self.layernorm1(inputs + attn_output)\r\n",
    "        ffn_output = self.ffn(out1)\r\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\r\n",
    "        return self.layernorm2(out1 + ffn_output)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "class TokenAndPositionEmbedding(layers.Layer):\r\n",
    "    def __init__(self, maxlen, vocab_size, embed_dim):\r\n",
    "        super(TokenAndPositionEmbedding, self).__init__()\r\n",
    "        self.token_emb = layers.Embedding(input_dim=vocab_size, output_dim=embed_dim)\r\n",
    "        self.pos_emb = layers.Embedding(input_dim=maxlen, output_dim=embed_dim)\r\n",
    "        \r\n",
    "    def call(self, x):\r\n",
    "        maxlen = tf.shape(x)[-1]\r\n",
    "        positions = tf.range(start=0, limit=maxlen, delta=1)\r\n",
    "        positions = self.pos_emb(positions)\r\n",
    "        x = self.token_emb(x)\r\n",
    "        return x + positions\r\n",
    "        #return x"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "class TokenEmbedding(layers.Layer):\r\n",
    "    def __init__(self, maxlen, vocab_size, embed_dim):\r\n",
    "        super(TokenEmbedding, self).__init__()\r\n",
    "        self.token_emb = layers.Embedding(input_dim=vocab_size, output_dim=embed_dim)\r\n",
    "        self.pos_emb = layers.Embedding(input_dim=maxlen, output_dim=embed_dim)\r\n",
    "        \r\n",
    "    def call(self, x):\r\n",
    "        maxlen = tf.shape(x)[-1]\r\n",
    "        positions = tf.range(start=0, limit=maxlen, delta=1)\r\n",
    "        positions = self.pos_emb(positions)\r\n",
    "        x = self.token_emb(x)\r\n",
    "        #return x + positions\r\n",
    "        return x"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "class TokenAndPositionAndDurationEmbedding(layers.Layer):\r\n",
    "    def __init__(self, maxlen, vocab_size, embed_dim):\r\n",
    "        super(TokenAndPositionAndDurationEmbedding, self).__init__()\r\n",
    "        self.token_emb = layers.Embedding(input_dim=vocab_size, output_dim=embed_dim)\r\n",
    "        self.dur_emb = layers.Embedding(input_dim=vocab_size, output_dim=embed_dim)\r\n",
    "        self.pos_emb = layers.Embedding(input_dim=maxlen, output_dim=embed_dim)\r\n",
    "\r\n",
    "    def call(self, x, y):\r\n",
    "        maxlen = tf.shape(x)[-1]\r\n",
    "        positions = tf.range(start=0, limit=maxlen, delta=1)\r\n",
    "        positions = self.pos_emb(positions)\r\n",
    "        x = self.token_emb(x)\r\n",
    "        y = self.dur_emb(y)\r\n",
    "        return x + y + positions"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "class BahdanauAttention(tf.keras.Model):\r\n",
    "  def __init__(self, units):\r\n",
    "    super(BahdanauAttention, self).__init__()\r\n",
    "    self.W1 = Dense(units)\r\n",
    "    self.W2 = Dense(units)\r\n",
    "    self.V = Dense(1)\r\n",
    "\r\n",
    "  def call(self, values, query): \r\n",
    "    # query shape == (batch_size, hidden size)\r\n",
    "    # hidden_with_time_axis shape == (batch_size, 1, hidden size)\r\n",
    "    hidden_with_time_axis = tf.expand_dims(query, 1)\r\n",
    "\r\n",
    "    # score shape == (batch_size, max_length, 1)\r\n",
    "    # we get 1 at the last axis because we are applying score to self.V\r\n",
    "    # the shape of the tensor before applying self.V is (batch_size, max_length, units)\r\n",
    "    score = self.V(tf.nn.tanh(\r\n",
    "        self.W1(values) + self.W2(hidden_with_time_axis)))\r\n",
    "\r\n",
    "    # attention_weights shape == (batch_size, max_length, 1)\r\n",
    "    attention_weights = tf.nn.softmax(score, axis=1)\r\n",
    "\r\n",
    "    # context_vector shape after sum == (batch_size, hidden_size)\r\n",
    "    context_vector = attention_weights * values\r\n",
    "    context_vector = tf.reduce_sum(context_vector, axis=1)\r\n",
    "\r\n",
    "    return context_vector, attention_weights"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def plot_graphs(history, string):\r\n",
    "  plt.plot(history.history[string])\r\n",
    "  plt.plot(history.history['val_' + string])\r\n",
    "  plt.xlabel(\"Epochs\")\r\n",
    "  plt.ylabel(string)\r\n",
    "  plt.legend([string, 'val_'+ string])\r\n",
    "  plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "avg_len=0\r\n",
    "with open('Window/Chatting_activity__AR.csv', newline='') as f:\r\n",
    "    reader = csv.reader(f)\r\n",
    "    data = list(reader)\r\n",
    "sentences1 = np.empty(len(data), dtype=object)\r\n",
    "\r\n",
    "for i in range(len(data)):\r\n",
    "    if(len(data[i])>maxlen):\r\n",
    "        maxlen =len(data[i])\r\n",
    "    avg_len +=len(data[i])\r\n",
    "    sentences1[i] =' '.join(data[i])\r\n",
    "print(len(sentences1))\r\n",
    "\r\n",
    "with open('Window/Presentation_activity__AR.csv', newline='') as f:\r\n",
    "    reader = csv.reader(f)\r\n",
    "    data = list(reader)\r\n",
    "sentences2 = np.empty(len(data), dtype=object)\r\n",
    "for i in range(len(data)):\r\n",
    "    if(len(data[i])>maxlen):\r\n",
    "        maxlen =len(data[i])\r\n",
    "    avg_len +=len(data[i])\r\n",
    "    sentences2[i] =' '.join(data[i])\r\n",
    "print(len(sentences2))\r\n",
    "\r\n",
    "\r\n",
    "with open('Window/Discussion_activity__AR.csv', newline='') as f:\r\n",
    "    reader = csv.reader(f)\r\n",
    "    data = list(reader)\r\n",
    "sentences3 = np.empty(len(data), dtype=object)\r\n",
    "for i in range(len(data)):\r\n",
    "    if(len(data[i])>maxlen):\r\n",
    "        maxlen =len(data[i])\r\n",
    "    avg_len +=len(data[i])\r\n",
    "    sentences3[i] =' '.join(data[i])\r\n",
    "print(len(sentences3))\r\n",
    "\r\n",
    "with open('Window/GroupStudy_activity__AR.csv', newline='') as f:\r\n",
    "    reader = csv.reader(f)\r\n",
    "    data = list(reader)\r\n",
    "sentences4 = np.empty(len(data), dtype=object)\r\n",
    "for i in range(len(data)):\r\n",
    "    if(len(data[i])>maxlen):\r\n",
    "        maxlen =len(data[i])\r\n",
    "    avg_len +=len(data[i])\r\n",
    "    sentences4[i] =' '.join(data[i])\r\n",
    "print(len(sentences4))\r\n",
    "\r\n",
    "#Combine all np arrays\r\n",
    "sentences =  np.concatenate((sentences1, sentences2))\r\n",
    "sentences =  np.concatenate((sentences, sentences3))\r\n",
    "sentences =  np.concatenate((sentences, sentences4))\r\n",
    "sentences = sentences.tolist()\r\n",
    "print(len(sentences))\r\n",
    "\r\n",
    "# Construct Y label\r\n",
    "labels = [] #  empty regular list\r\n",
    "for i in range(len(sentences1)):\r\n",
    "    labels.append(0*np.ones((1)))\r\n",
    "for i in range(len(sentences2)):\r\n",
    "    labels.append(1*np.ones((1)))\r\n",
    "for i in range(len(sentences3)):\r\n",
    "    labels.append(2*np.ones((1)))\r\n",
    "for i in range(len(sentences4)):\r\n",
    "    labels.append(3*np.ones((1)))\r\n",
    "#np_array = np.array(arr)  # transformed to a numpy array\r\n",
    "#y_df = pd.DataFrame({'': np_array[:, 0]})\r\n",
    "#maxlen = 100\r\n",
    "#maxlen = (int)(avg_len/len(sentences))\r\n",
    "print(len(labels))\r\n",
    "print(maxlen)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "with open('Window/Chatting_duration_AR_v2.csv', newline='') as f:\r\n",
    "    reader = csv.reader(f)\r\n",
    "    data = list(reader)\r\n",
    "duration1 = np.empty(len(data), dtype=object)\r\n",
    "for i in range(len(data)):\r\n",
    "    duration1[i] =' '.join(data[i])\r\n",
    "print(len(duration1))\r\n",
    "\r\n",
    "with open('Window/Presentation_duration_AR_v2.csv', newline='') as f:\r\n",
    "    reader = csv.reader(f)\r\n",
    "    data = list(reader)\r\n",
    "duration2 = np.empty(len(data), dtype=object)\r\n",
    "for i in range(len(data)):\r\n",
    "    duration2[i] =' '.join(data[i])\r\n",
    "print(len(duration2))\r\n",
    "\r\n",
    "\r\n",
    "with open('Window/Discussion_duration_AR_v2.csv', newline='') as f:\r\n",
    "    reader = csv.reader(f)\r\n",
    "    data = list(reader)\r\n",
    "duration3 = np.empty(len(data), dtype=object)\r\n",
    "for i in range(len(data)):\r\n",
    "    duration3[i] =' '.join(data[i])\r\n",
    "print(len(duration3))\r\n",
    "\r\n",
    "with open('Window/GroupStudy_duration_AR_v2.csv', newline='') as f:\r\n",
    "    reader = csv.reader(f)\r\n",
    "    data = list(reader)\r\n",
    "duration4 = np.empty(len(data), dtype=object)\r\n",
    "for i in range(len(data)):\r\n",
    "    duration4[i] =' '.join(data[i])\r\n",
    "print(len(duration4))\r\n",
    "\r\n",
    "#Combine all np arrays\r\n",
    "duration =  np.concatenate((duration1,duration2))\r\n",
    "duration =  np.concatenate((duration, duration3))\r\n",
    "duration =  np.concatenate((duration, duration4))\r\n",
    "duration =  duration.tolist()\r\n",
    "print(len(duration))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Pre-defined parameters and models"
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "plaintext"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}