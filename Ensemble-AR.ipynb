{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble learning for activity recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Develop an mlp for blobs dataset\n",
    "# Reference from https://machinelearningmastery.com/stacking-ensemble-for-deep-learning-neural-networks/\n",
    "from sklearn.datasets import make_blobs\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from matplotlib import pyplot\n",
    "from os import makedirs\n",
    "from tensorflow.keras.models import load_model\n",
    "from numpy import dstack \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate 2d classification dataset\n",
    "X, y = make_blobs(n_samples=1100, centers=3, n_features=2, cluster_std=2, random_state=2)\n",
    "# one hot encode output variable\n",
    "y = to_categorical(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 2) (1000, 2)\n"
     ]
    }
   ],
   "source": [
    "# split into train and test\n",
    "n_train = 100\n",
    "trainX, testX = X[:n_train, :], X[n_train:, :]\n",
    "trainy, testy = y[:n_train], y[n_train:]\n",
    "print(trainX.shape, testX.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit model on dataset\n",
    "def fit_model(trainX, trainy):\n",
    "\t# define model\n",
    "\tmodel = Sequential()\n",
    "\tmodel.add(Dense(25, input_dim=2, activation='relu'))\n",
    "\tmodel.add(Dense(3, activation='softmax'))\n",
    "\tmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\t# fit model\n",
    "\tmodel.fit(trainX, trainy, epochs=500, verbose=0)\n",
    "\treturn model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create directory for models\n",
    "#akedirs('models')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">Saved models/model_1.h5\n",
      ">Saved models/model_2.h5\n",
      ">Saved models/model_3.h5\n",
      ">Saved models/model_4.h5\n",
      ">Saved models/model_5.h5\n"
     ]
    }
   ],
   "source": [
    "# fit and save models\n",
    "n_members = 5\n",
    "for i in range(n_members):\n",
    "\t# fit model\n",
    "\tmodel = fit_model(trainX, trainy)\n",
    "\t# save model\n",
    "\tfilename = 'models/model_' + str(i + 1) + '.h5'\n",
    "\tmodel.save(filename)\n",
    "\tprint('>Saved %s' % filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load models from file\n",
    "def load_all_models(n_models):\n",
    "\tall_models = list()\n",
    "\tfor i in range(n_models):\n",
    "\t\t# define filename for this ensemble\n",
    "\t\tfilename = 'models/model_' + str(i + 1) + '.h5'\n",
    "\t\t# load model from file\n",
    "\t\tmodel = load_model(filename)\n",
    "\t\t# add to list of members\n",
    "\t\tall_models.append(model)\n",
    "\t\tprint('>loaded %s' % filename)\n",
    "\treturn all_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create stacked model input dataset as outputs from the ensemble\n",
    "def stacked_dataset(members, inputX):\n",
    "\tstackX = None\n",
    "\tfor model in members:\n",
    "\t\t# make prediction\n",
    "\t\tyhat = model.predict(inputX, verbose=0)\n",
    "\t\t# stack predictions into [rows, members, probabilities]\n",
    "\t\tif stackX is None:\n",
    "\t\t\tstackX = yhat\n",
    "\t\telse:\n",
    "\t\t\tstackX = dstack((stackX, yhat))\n",
    "\t# flatten predictions to [rows, members x probabilities]\n",
    "\tstackX = stackX.reshape((stackX.shape[0], stackX.shape[1]*stackX.shape[2]))\n",
    "\treturn stackX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit a model based on the outputs from the ensemble members\n",
    "def fit_stacked_model(members, inputX, inputy):\n",
    "\t# create dataset using ensemble\n",
    "\tstackedX = stacked_dataset(members, inputX)\n",
    "\t# fit standalone model\n",
    "\tmodel = LogisticRegression()\n",
    "\tmodel.fit(stackedX, inputy)\n",
    "\treturn model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a prediction with the stacked model\n",
    "def stacked_prediction(members, model, inputX):\n",
    "\t# create dataset using ensemble\n",
    "\tstackedX = stacked_dataset(members, inputX)\n",
    "\t# make a prediction\n",
    "\tyhat = model.predict(stackedX)\n",
    "\treturn yhat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">loaded models/model_1.h5\n",
      ">loaded models/model_2.h5\n",
      ">loaded models/model_3.h5\n",
      ">loaded models/model_4.h5\n",
      ">loaded models/model_5.h5\n",
      "Loaded 5 models\n"
     ]
    }
   ],
   "source": [
    "# load all models\n",
    "n_members = 5\n",
    "members = load_all_models(n_members)\n",
    "print('Loaded %d models' % len(members))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 2) (1000, 2)\n"
     ]
    }
   ],
   "source": [
    "# generate 2d classification dataset\n",
    "X, y = make_blobs(n_samples=1100, centers=3, n_features=2, cluster_std=2, random_state=2)\n",
    "# split into train and test\n",
    "n_train = 100\n",
    "trainX, testX = X[:n_train, :], X[n_train:, :]\n",
    "trainy, testy = y[:n_train], y[n_train:]\n",
    "print(trainX.shape, testX.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Accuracy: 0.811\n",
      "Model Accuracy: 0.807\n",
      "Model Accuracy: 0.810\n",
      "Model Accuracy: 0.816\n",
      "Model Accuracy: 0.808\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'LogisticRegression' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-31-5430f50296fd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Model Accuracy: %.3f'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0macc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;31m# fit stacked model using the ensemble\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfit_stacked_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmembers\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtestX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtesty\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;31m# evaluate model on test set\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[0myhat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstacked_prediction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmembers\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtestX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-27-40f89624dcce>\u001b[0m in \u001b[0;36mfit_stacked_model\u001b[1;34m(members, inputX, inputy)\u001b[0m\n\u001b[0;32m      4\u001b[0m         \u001b[0mstackedX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstacked_dataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmembers\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[1;31m# fit standalone model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m         \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLogisticRegression\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstackedX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'LogisticRegression' is not defined"
     ]
    }
   ],
   "source": [
    "# evaluate standalone models on test dataset\n",
    "for model in members:\n",
    "\ttesty_enc = to_categorical(testy)\n",
    "\t_, acc = model.evaluate(testX, testy_enc, verbose=0)\n",
    "\tprint('Model Accuracy: %.3f' % acc)\n",
    "# fit stacked model using the ensemble\n",
    "model = fit_stacked_model(members, testX, testy)\n",
    "# evaluate model on test set\n",
    "yhat = stacked_prediction(members, model, testX)\n",
    "acc = accuracy_score(testy, yhat)\n",
    "print('Stacked Test Accuracy: %.3f' % acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define stacked model from multiple member input models\n",
    "def define_stacked_model(members):\n",
    "\t# update all layers in all models to not be trainable\n",
    "\tfor i in range(len(members)):\n",
    "\t\tmodel = members[i]\n",
    "\t\tfor layer in model.layers:\n",
    "\t\t\t# make not trainable\n",
    "\t\t\tlayer.trainable = False\n",
    "\t\t\t# rename to avoid 'unique layer name' issue\n",
    "\t\t\tlayer._name = 'ensemble_' + str(i+1) + '_' + layer.name\n",
    "\t# define multi-headed input\n",
    "\tensemble_visible = [model.input for model in members]\n",
    "\t# concatenate merge output from each model\n",
    "\tensemble_outputs = [model.output for model in members]\n",
    "\tmerge = concatenate(ensemble_outputs)\n",
    "\thidden = Dense(10, activation='relu')(merge)\n",
    "\toutput = Dense(3, activation='softmax')(hidden)\n",
    "\tmodel = Model(inputs=ensemble_visible, outputs=output)\n",
    "\t# plot graph of ensemble\n",
    "\tplot_model(model, show_shapes=True, to_file='model_graph.png')\n",
    "\t# compile\n",
    "\tmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\treturn model\n",
    " \n",
    "# fit a stacked model\n",
    "def fit_stacked_model(model, inputX, inputy):\n",
    "\t# prepare input data\n",
    "\tX = [inputX for _ in range(len(model.input))]\n",
    "\t# encode output data\n",
    "\tinputy_enc = to_categorical(inputy)\n",
    "\t# fit model\n",
    "\tmodel.fit(X, inputy_enc, epochs=300, verbose=0)\n",
    " \n",
    "# make a prediction with a stacked model\n",
    "def predict_stacked_model(model, inputX):\n",
    "\t# prepare input data\n",
    "\tX = [inputX for _ in range(len(model.input))]\n",
    "\t# make prediction\n",
    "\treturn model.predict(X, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# generate 2d classification dataset\n",
    "X, y = make_blobs(n_samples=1100, centers=3, n_features=2, cluster_std=2, random_state=2)\n",
    "# split into train and test\n",
    "n_train = 100\n",
    "trainX, testX = X[:n_train, :], X[n_train:, :]\n",
    "trainy, testy = y[:n_train], y[n_train:]\n",
    "print(trainX.shape, testX.shape)\n",
    "# load all models\n",
    "n_members = 5\n",
    "members = load_all_models(n_members)\n",
    "print('Loaded %d models' % len(members))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Integrated Stacking Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.metrics import accuracy_score\n",
    "from keras.models import load_model\n",
    "from keras.utils import to_categorical\n",
    "from keras.utils import plot_model\n",
    "from keras.models import Model\n",
    "from keras.layers import Input\n",
    "from keras.layers import Dense\n",
    "from keras.layers.merge import concatenate\n",
    "from numpy import argmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load models from file\n",
    "def load_all_models(n_models):\n",
    "\tall_models = list()\n",
    "\tfor i in range(n_models):\n",
    "\t\t# define filename for this ensemble\n",
    "\t\tfilename = 'models/model_' + str(i + 1) + '.h5'\n",
    "\t\t# load model from file\n",
    "\t\tmodel = load_model(filename)\n",
    "\t\t# add to list of members\n",
    "\t\tall_models.append(model)\n",
    "\t\tprint('>loaded %s' % filename)\n",
    "\treturn all_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define stacked model from multiple member input models\n",
    "def define_stacked_model(members):\n",
    "\t# update all layers in all models to not be trainable\n",
    "\tfor i in range(len(members)):\n",
    "\t\tmodel = members[i]\n",
    "\t\tfor layer in model.layers:\n",
    "\t\t\t# make not trainable\n",
    "\t\t\tlayer.trainable = False\n",
    "\t\t\t# rename to avoid 'unique layer name' issue\n",
    "\t\t\tlayer._name = 'ensemble_' + str(i+1) + '_' + layer.name\n",
    "\t# define multi-headed input\n",
    "\tensemble_visible = [model.input for model in members]\n",
    "\t# concatenate merge output from each model\n",
    "\tensemble_outputs = [model.output for model in members]\n",
    "\tmerge = concatenate(ensemble_outputs)\n",
    "\thidden = Dense(10, activation='relu')(merge)\n",
    "\toutput = Dense(3, activation='softmax')(hidden)\n",
    "\tmodel = Model(inputs=ensemble_visible, outputs=output)\n",
    "\t# plot graph of ensemble\n",
    "#\tplot_model(model, show_shapes=True, to_file='model_graph.png')\n",
    "\t# compile\n",
    "\tmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\treturn model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit a stacked model\n",
    "def fit_stacked_model(model, inputX, inputy):\n",
    "\t# prepare input data\n",
    "\tX = [inputX for _ in range(len(model.input))]\n",
    "\t# encode output data\n",
    "\tinputy_enc = to_categorical(inputy)\n",
    "\t# fit model\n",
    "\tmodel.fit(X, inputy_enc, epochs=300, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a prediction with a stacked model\n",
    "def predict_stacked_model(model, inputX):\n",
    "\t# prepare input data\n",
    "\tX = [inputX for _ in range(len(model.input))]\n",
    "\t# make prediction\n",
    "\treturn model.predict(X, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate 2d classification dataset\n",
    "X, y = make_blobs(n_samples=1100, centers=3, n_features=2, cluster_std=2, random_state=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into train and test\n",
    "n_train = 100\n",
    "trainX, testX = X[:n_train, :], X[n_train:, :]\n",
    "trainy, testy = y[:n_train], y[n_train:]\n",
    "print(trainX.shape, testX.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load all models\n",
    "n_members = 5\n",
    "members = load_all_models(n_members)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Loaded %d models' % len(members))\n",
    "# define ensemble model\n",
    "stacked_model = define_stacked_model(members)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit stacked model on test dataset\n",
    "fit_stacked_model(stacked_model, testX, testy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make predictions and evaluate\n",
    "yhat = predict_stacked_model(stacked_model, testX)\n",
    "yhat = argmax(yhat, axis=1)\n",
    "acc = accuracy_score(testy, yhat)\n",
    "print('Stacked Test Accuracy: %.3f' % acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM and CNN for sequence classification in the IMDB dataset\n",
    "import numpy\n",
    "from keras.datasets import imdb\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    "layers.concatenate\n",
    "from keras.preprocessing import sequence\n",
    "# fix random seed for reproducibility\n",
    "numpy.random.seed(7)\n",
    "# load the dataset but only keep the top n words, zero the rest\n",
    "top_words = 5000\n",
    "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=top_words)\n",
    "# truncate and pad input sequences\n",
    "max_review_length = 500\n",
    "X_train = sequence.pad_sequences(X_train, maxlen=max_review_length)\n",
    "X_test = sequence.pad_sequences(X_test, maxlen=max_review_length)\n",
    "# create the model\n",
    "embedding_vecor_length = 32\n",
    "model = Sequential()\n",
    "model.add(Embedding(top_words, embedding_vecor_length, input_length=max_review_length))\n",
    "model.add(Conv1D(filters=32, kernel_size=3, padding='same', activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(LSTM(100))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())\n",
    "model.fit(X_train, y_train, epochs=3, batch_size=64)\n",
    "# Final evaluation of the model\n",
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[0.]]\n",
      "\n",
      " [[1.]]\n",
      "\n",
      " [[2.]]\n",
      "\n",
      " [[3.]]\n",
      "\n",
      " [[4.]]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "arr = [] #  empty regular list\n",
    "for i in range(len(dataframe1)):\n",
    "    arr.append(1*np.ones((1)))\n",
    "for i in range(len(dataframe2)):\n",
    "    arr.append(2*np.ones((1)))\n",
    "for i in range(len(dataframe3)):\n",
    "    arr.append(3*np.ones((1)))\n",
    "for i in range(len(dataframe4)):\n",
    "    arr.append(4*np.ones((1)))\n",
    "big_np_array = np.array(arr)  # transformed to a numpy array\n",
    "print(big_np_array)\n",
    "pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf2.0-gpu",
   "language": "python",
   "name": "tf2.0-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
