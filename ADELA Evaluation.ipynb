{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble learning for activity recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Set libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.debugging.set_log_device_placement(True)\n",
    "import keras as keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Dropout, Activation\n",
    "from keras import optimizers\n",
    "from keras.utils import to_categorical\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from random import randint\n",
    "from numpy import array\n",
    "from numpy import argmax\n",
    "from pandas import DataFrame\n",
    "from pandas import concat\n",
    "from keras.layers import GRU, LSTM, Activation, Bidirectional\n",
    "from keras.layers import Dense, Embedding\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.layers import Concatenate, Flatten \n",
    "from keras.layers import Dense, concatenate\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "# stacked generalization with linear meta model on blobs dataset\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from keras.models import load_model\n",
    "from numpy import dstack\n",
    "#from attention import Attention\n",
    "\n",
    "# univariate cnn lstm example\n",
    "from numpy import array\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    "from keras.layers import SpatialDropout1D\n",
    "from keras.layers import ConvLSTM2D\n",
    "from scipy.optimize import differential_evolution\n",
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from matplotlib import pyplot\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from numpy import array\n",
    "from numpy import argmax\n",
    "from numpy import tensordot\n",
    "from numpy.linalg import norm\n",
    "from itertools import product\n",
    "from keras.layers import Dense, Embedding, Bidirectional, LSTM, Concatenate, Dropout\n",
    "from keras import Input, Model\n",
    "from keras import optimizers\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset with pandas\n",
    "dataframe1 = pd.read_csv('data/input_v2/Presentation_activity.csv', sep=',', header=None)\n",
    "dataframe2 = pd.read_csv('data/input_v2/Chatting_activity.csv', sep=',', header=None)\n",
    "dataframe3 = pd.read_csv('data/input_v2/Discussion_activity.csv', sep=',', header=None)\n",
    "dataframe4 = pd.read_csv('data/input_v2/GroupStudy_activity.csv', sep=',', header=None)\n",
    "\n",
    "# Show how many episodes \n",
    "#print(len(dataframe1))\n",
    "#print(len(dataframe2))\n",
    "#print(len(dataframe3))\n",
    "#print(len(dataframe4))\n",
    "\n",
    "#Combine all dataframe\n",
    "dataframe = dataframe1.append(dataframe2,ignore_index=True)\n",
    "dataframe = dataframe.append(dataframe3,ignore_index=True)\n",
    "dataframe = dataframe.append(dataframe4,ignore_index=True)\n",
    "\n",
    "#Change NaN data to 0\n",
    "dataframe = dataframe.fillna(0)\n",
    "dataset = dataframe.values\n",
    "\n",
    "#288, 6\n",
    "#Seperate input and do one-hot encoding\n",
    "#X=to_categorical(dataset)\n",
    "X=dataset\n",
    "print(\"The shape of X:\", X.shape)\n",
    "\n",
    "# Construct Y label\n",
    "arr = [] #  empty regular list\n",
    "for i in range(len(dataframe1)):\n",
    "    arr.append(0*np.ones((1)))\n",
    "for i in range(len(dataframe2)):\n",
    "    arr.append(1*np.ones((1)))\n",
    "for i in range(len(dataframe3)):\n",
    "    arr.append(2*np.ones((1)))\n",
    "for i in range(len(dataframe4)):\n",
    "    arr.append(3*np.ones((1)))\n",
    "np_array = np.array(arr)  # transformed to a numpy array\n",
    "y_df = pd.DataFrame({'': np_array[:, 0]})\n",
    "\n",
    "Y=to_categorical(y_df)\n",
    "#Y=y_df\n",
    "print(\"The shape of Y:\", Y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Set parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 50\n",
    "batch_size = 64\n",
    "length=724\n",
    "max_len=length\n",
    "n_members = 4\n",
    "n_features = 1\n",
    "n_seq = 4\n",
    "n_steps = 181\n",
    "file_name ='Result_Weighted.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Construct base models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model1_made():\n",
    "    model1 = tf.keras.Sequential()\n",
    "    model1.add(tf.keras.layers.Embedding(100, 128, input_length=max_len, mask_zero = True))\n",
    "    model1.add(tf.keras.layers.SpatialDropout1D(0.2))\n",
    "    model1.add(tf.keras.layers.Bidirectional(LSTM(64, dropout=0.2, recurrent_dropout=0.2)))\n",
    "    model1.add(tf.keras.layers.Dense(4, activation='softmax'))\n",
    "    model1.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    "    return model1\n",
    "\n",
    "def model2_made():\n",
    "    model2 = tf.keras.Sequential()\n",
    "    model2.add(tf.keras.layers.Embedding(100, 128, input_length=max_len, mask_zero = True))\n",
    "    model2.add(tf.keras.layers.SpatialDropout1D(0.2))  \n",
    "    model2.add(tf.keras.layers.LSTM(64, dropout=0.2, recurrent_dropout=0.2))\n",
    "    model2.add(tf.keras.layers.Dense(4, activation='softmax'))\n",
    "    model2.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    "    return model2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model3_made():\n",
    "    model3 = Sequential()\n",
    "    model3.add(Embedding(8, 128, input_length=max_len, mask_zero = True))\n",
    "    model3.add(SpatialDropout1D(0.2))  \n",
    "    model3.add(GRU(100, dropout=0.2, recurrent_dropout=0.2))\n",
    "    model3.add(Dense(4, activation='softmax'))\n",
    "    model3.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    "    return model3\n",
    "def model4_made():\n",
    "    model4 = Sequential()\n",
    "    model4.add(Embedding(8, 128, input_length=max_len, mask_zero = True))\n",
    "    model4.add(SpatialDropout1D(0.2))  \n",
    "    model4.add(Bidirectional(GRU(100, dropout=0.2, recurrent_dropout=0.2)))\n",
    "    model4.add(Dense(4, activation='softmax'))\n",
    "    model4.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    "    return model4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model5_made():#CNN+BiLSTM\n",
    "    # define model\n",
    "    model5 = Sequential()\n",
    "    model5.add(TimeDistributed(Conv1D(filters=4, kernel_size=1, activation='relu'), \n",
    "                               input_shape=(None, n_steps, n_features)))\n",
    "    model5.add(TimeDistributed(MaxPooling1D(pool_size=4)))\n",
    "    model5.add(TimeDistributed(Flatten()))\n",
    "    model5.add(Bidirectional(LSTM(64, dropout=0.2, recurrent_dropout=0.2)))\n",
    "    model5.add(Dense(4, activation='softmax'))\n",
    "    model5.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    "    return model5\n",
    "def model6_made():#CNN+LSTM\n",
    "    # define model\n",
    "    model = Sequential()\n",
    "    model.add(TimeDistributed(Conv1D(filters=4, kernel_size=1, activation='relu'), \n",
    "                              input_shape=(None, n_steps, n_features)))\n",
    "    model.add(TimeDistributed(MaxPooling1D(pool_size=4)))\n",
    "    model.add(TimeDistributed(Flatten()))\n",
    "    model.add(LSTM(64, dropout=0.2, recurrent_dropout=0.2))\n",
    "    model.add(Dense(4, activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def model7_made():#ConvLSTM\n",
    "    # define model\n",
    "    model = Sequential()\n",
    "    model.add(ConvLSTM2D(filters=4, kernel_size=(1,2), activation='relu', \n",
    "                         input_shape=(n_seq, 1, n_steps, n_features)))\n",
    "    model.add(TimeDistributed(Flatten()))\n",
    "    model.add(Dense(4, activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    "    #model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BahdanauAttention(tf.keras.Model):\n",
    "  def __init__(self, units):\n",
    "    super(BahdanauAttention, self).__init__()\n",
    "    self.W1 = Dense(units)\n",
    "    self.W2 = Dense(units)\n",
    "    self.V = Dense(1)\n",
    "\n",
    "  def call(self, values, query): # 단, key와 value는 같음\n",
    "    # query shape == (batch_size, hidden size)\n",
    "    # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
    "    # score 계산을 위해 뒤에서 할 덧셈을 위해서 차원을 변경해줍니다.\n",
    "    hidden_with_time_axis = tf.expand_dims(query, 1)\n",
    "\n",
    "    # score shape == (batch_size, max_length, 1)\n",
    "    # we get 1 at the last axis because we are applying score to self.V\n",
    "    # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
    "    score = self.V(tf.nn.tanh(\n",
    "        self.W1(values) + self.W2(hidden_with_time_axis)))\n",
    "\n",
    "    # attention_weights shape == (batch_size, max_length, 1)\n",
    "    attention_weights = tf.nn.softmax(score, axis=1)\n",
    "\n",
    "    # context_vector shape after sum == (batch_size, hidden_size)\n",
    "    context_vector = attention_weights * values\n",
    "    context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "\n",
    "    return context_vector, attention_weights\n",
    "\n",
    "def model8_made(): #BiLSTM with attention\n",
    "    sequence_input = tf.keras.Input(shape=(max_len,), dtype='int32')\n",
    "    embedded_sequences = tf.keras.layers.Embedding(100, 128, input_length=max_len, mask_zero = True)(sequence_input)\n",
    "    lstm = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64, dropout=0.5, return_sequences = True))(embedded_sequences)\n",
    "    lstm, forward_h, forward_c, backward_h, backward_c = tf.keras.layers.Bidirectional \\\n",
    "  (tf.keras.layers.LSTM(64, dropout=0.5, return_sequences=True, return_state=True))(lstm)\n",
    "    print(lstm.shape, forward_h.shape, forward_c.shape, backward_h.shape, backward_c.shape)\n",
    "    state_h = tf.keras.layers.Concatenate()([forward_h, backward_h]) # 은닉 상태\n",
    "    state_c = tf.keras.layers.Concatenate()([forward_c, backward_c]) # 셀 상태\n",
    "    attention = BahdanauAttention(64) # 가중치 크기 정의\n",
    "    context_vector, attention_weights = attention(lstm, state_h)\n",
    "    dense1 = tf.keras.layers.Dense(20, activation=\"relu\")(context_vector)\n",
    "    dropout = tf.keras.layers.Dropout(0.2)(dense1)\n",
    "    output = tf.keras.layers.Dense(4, activation=\"softmax\")(dropout)\n",
    "    model = tf.keras.Model(inputs=sequence_input, outputs=output)\n",
    "    model.compile(loss='sparse_categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def model9_made(): #LSTM with attention\n",
    "    sequence_input = tf.keras.Input(shape=(max_len,), dtype='int32')\n",
    "    embedded_sequences = tf.keras.layers.Embedding(100, 128, input_length=max_len, mask_zero = True)(sequence_input)\n",
    "    lstm = tf.keras.layers.LSTM(64, dropout=0.5, return_sequences = True)(embedded_sequences)\n",
    "    lstm, forward_h, forward_c = tf.keras.layers.LSTM(64, dropout=0.5, return_sequences=True, return_state=True)(lstm)\n",
    "    print(lstm.shape, forward_h.shape, forward_c.shape)\n",
    "    attention = BahdanauAttention(64) # 가중치 크기 정의\n",
    "    context_vector, attention_weights = attention(lstm, forward_h)\n",
    "    dense1 = tf.keras.layers.Dense(20, activation=\"relu\")(context_vector)\n",
    "    dropout = tf.keras.layers.Dropout(0.2)(dense1)\n",
    "    output = tf.keras.layers.Dense(4, activation=\"softmax\")(dropout)\n",
    "    model = tf.keras.Model(inputs=sequence_input, outputs=output)\n",
    "    model.compile(loss='sparse_categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contstruct evalution methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_evaluate1(X_train,Y_train,model,X_test, Y_test):\n",
    "    #tf.debugging.set_log_device_placement(True)\n",
    "    history = model.fit(X_train, Y_train, epochs=epochs, batch_size=batch_size,validation_split=0.1)\n",
    "    # evaluate model\n",
    "    accr = model.evaluate(X_test, Y_test, batch_size=batch_size, verbose=0)\n",
    "    print('Test set\\n  Loss: {:0.3f}\\n  Accuracy: {:0.3f}'.format(accr[0],accr[1]))\n",
    "    #plt.title('Loss')\n",
    "    #plt.plot(history.history['loss'], label='train')\n",
    "    #plt.plot(history.history['val_loss'], label='test')\n",
    "    #plt.legend()\n",
    "    #plt.show();\n",
    "\n",
    "    #plt.title('Accuracy')\n",
    "    #plt.plot(history.history['accuracy'], label='train')\n",
    "    #plt.plot(history.history['val_accuracy'], label='test')\n",
    "    #plt.legend()\n",
    "    #plt.show();\n",
    "\n",
    "    yhat=model.predict(X_test,verbose=0)\n",
    "    write_rs(argmax(Y_test,axis=1), argmax(yhat,axis=1))\n",
    "    #argmax(yhat1, axis=1)\n",
    "    return [accr, yhat]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_evaluate2(X_train,Y_train,model,X_test, Y_test):\n",
    "    # fit model\n",
    "    X_train1 = X_train.reshape((X_train.shape[0], n_seq, n_steps, n_features))\n",
    "    model.fit(X_train1, Y_train, epochs=epochs, batch_size=batch_size,validation_split=0.1)\n",
    "    # demonstrate prediction    \n",
    "    X_test1 = X_test.reshape((X_test.shape[0], n_seq, n_steps, n_features))\n",
    "    accr = model.evaluate(X_test1, Y_test, batch_size=batch_size, verbose=0)\n",
    "    print('Test set\\n  Loss: {:0.3f}\\n  Accuracy: {:0.3f}'.format(accr[0],accr[1]))\n",
    "    yhat = model.predict(X_test1, verbose=0)\n",
    "    write_rs(argmax(Y_test,axis=1), argmax(yhat,axis=1))\n",
    "    \n",
    "    return [accr, yhat]\n",
    "def model_evaluate3(X_train,Y_train,model,X_test, Y_test):    \n",
    "    # fit model\n",
    "    X_train2 = X_train.reshape((X_train.shape[0], n_seq, 1, n_steps, n_features))\n",
    "    Y_train2 = Y_train.reshape((Y_train.shape[0], 1, Y_train.shape[1]))\n",
    "    model.fit(X_train2, Y_train2, epochs=epochs, batch_size=batch_size,validation_split=0.1)\n",
    "    # demonstrate prediction    \n",
    "    X_test2 = X_test.reshape((X_test.shape[0], n_seq, 1, n_steps, n_features))\n",
    "    Y_test2 = Y_test.reshape((Y_test.shape[0], 1, Y_test.shape[1]))\n",
    "    accr = model.evaluate(X_test2, Y_test2, batch_size=batch_size, verbose=0)\n",
    "    print('Test set\\n  Loss: {:0.3f}\\n  Accuracy: {:0.3f}'.format(accr[0],accr[1]))\n",
    "        \n",
    "    yhat = model.predict(X_test2, verbose=0)\n",
    "    yhat = np.reshape(yhat,(len(yhat),4))\n",
    "    write_rs(argmax(Y_test,axis=1), argmax(yhat,axis=1))\n",
    "    return [accr, yhat]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf2.0-gpu",
   "language": "python",
   "name": "tf2.0-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
