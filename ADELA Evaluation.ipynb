{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble learning for activity recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Set libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.debugging.set_log_device_placement(True)\n",
    "import keras as keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Dropout, Activation\n",
    "from keras import optimizers\n",
    "from keras.utils import to_categorical\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from random import randint\n",
    "from numpy import array\n",
    "from numpy import argmax\n",
    "from pandas import DataFrame\n",
    "from pandas import concat\n",
    "from keras.layers import GRU, LSTM, Activation, Bidirectional\n",
    "from keras.layers import Dense, Embedding\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.layers import Concatenate, Flatten \n",
    "from keras.layers import Dense, concatenate\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "# stacked generalization with linear meta model on blobs dataset\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from keras.models import load_model\n",
    "from numpy import dstack\n",
    "#from attention import Attention\n",
    "\n",
    "# univariate cnn lstm example\n",
    "from numpy import array\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    "from keras.layers import SpatialDropout1D\n",
    "from keras.layers import ConvLSTM2D\n",
    "from scipy.optimize import differential_evolution\n",
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from matplotlib import pyplot\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from numpy import array\n",
    "from numpy import argmax\n",
    "from numpy import tensordot\n",
    "from numpy.linalg import norm\n",
    "from itertools import product\n",
    "from keras.layers import Dense, Embedding, Bidirectional, LSTM, Concatenate, Dropout\n",
    "from keras import Input, Model\n",
    "from keras import optimizers\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset with pandas\n",
    "dataframe1 = pd.read_csv('data/input_v2/Presentation_activity.csv', sep=',', header=None)\n",
    "dataframe2 = pd.read_csv('data/input_v2/Chatting_activity.csv', sep=',', header=None)\n",
    "dataframe3 = pd.read_csv('data/input_v2/Discussion_activity.csv', sep=',', header=None)\n",
    "dataframe4 = pd.read_csv('data/input_v2/GroupStudy_activity.csv', sep=',', header=None)\n",
    "\n",
    "# Show how many episodes \n",
    "#print(len(dataframe1))\n",
    "#print(len(dataframe2))\n",
    "#print(len(dataframe3))\n",
    "#print(len(dataframe4))\n",
    "\n",
    "#Combine all dataframe\n",
    "dataframe = dataframe1.append(dataframe2,ignore_index=True)\n",
    "dataframe = dataframe.append(dataframe3,ignore_index=True)\n",
    "dataframe = dataframe.append(dataframe4,ignore_index=True)\n",
    "\n",
    "#Change NaN data to 0\n",
    "dataframe = dataframe.fillna(0)\n",
    "dataset = dataframe.values\n",
    "\n",
    "#288, 6\n",
    "#Seperate input and do one-hot encoding\n",
    "#X=to_categorical(dataset)\n",
    "X=dataset\n",
    "print(\"The shape of X:\", X.shape)\n",
    "\n",
    "# Construct Y label\n",
    "arr = [] #  empty regular list\n",
    "for i in range(len(dataframe1)):\n",
    "    arr.append(0*np.ones((1)))\n",
    "for i in range(len(dataframe2)):\n",
    "    arr.append(1*np.ones((1)))\n",
    "for i in range(len(dataframe3)):\n",
    "    arr.append(2*np.ones((1)))\n",
    "for i in range(len(dataframe4)):\n",
    "    arr.append(3*np.ones((1)))\n",
    "np_array = np.array(arr)  # transformed to a numpy array\n",
    "y_df = pd.DataFrame({'': np_array[:, 0]})\n",
    "\n",
    "Y=to_categorical(y_df)\n",
    "#Y=y_df\n",
    "print(\"The shape of Y:\", Y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Set parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 50\n",
    "batch_size = 64\n",
    "length=724\n",
    "max_len=length\n",
    "n_members = 4\n",
    "n_features = 1\n",
    "n_seq = 4\n",
    "n_steps = 181\n",
    "file_name ='Result_Weighted.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Construct base models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model1_made():\n",
    "    model1 = tf.keras.Sequential()\n",
    "    model1.add(tf.keras.layers.Embedding(100, 128, input_length=max_len, mask_zero = True))\n",
    "    model1.add(tf.keras.layers.SpatialDropout1D(0.2))\n",
    "    model1.add(tf.keras.layers.Bidirectional(LSTM(64, dropout=0.2, recurrent_dropout=0.2)))\n",
    "    model1.add(tf.keras.layers.Dense(4, activation='softmax'))\n",
    "    model1.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    "    return model1\n",
    "\n",
    "def model2_made():\n",
    "    model2 = tf.keras.Sequential()\n",
    "    model2.add(tf.keras.layers.Embedding(100, 128, input_length=max_len, mask_zero = True))\n",
    "    model2.add(tf.keras.layers.SpatialDropout1D(0.2))  \n",
    "    model2.add(tf.keras.layers.LSTM(64, dropout=0.2, recurrent_dropout=0.2))\n",
    "    model2.add(tf.keras.layers.Dense(4, activation='softmax'))\n",
    "    model2.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    "    return model2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create directory for models\n",
    "#akedirs('models')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit and save models\n",
    "n_members = 5\n",
    "for i in range(n_members):\n",
    "\t# fit model\n",
    "\tmodel = fit_model(trainX, trainy)\n",
    "\t# save model\n",
    "\tfilename = 'models/model_' + str(i + 1) + '.h5'\n",
    "\tmodel.save(filename)\n",
    "\tprint('>Saved %s' % filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load models from file\n",
    "def load_all_models(n_models):\n",
    "\tall_models = list()\n",
    "\tfor i in range(n_models):\n",
    "\t\t# define filename for this ensemble\n",
    "\t\tfilename = 'models/model_' + str(i + 1) + '.h5'\n",
    "\t\t# load model from file\n",
    "\t\tmodel = load_model(filename)\n",
    "\t\t# add to list of members\n",
    "\t\tall_models.append(model)\n",
    "\t\tprint('>loaded %s' % filename)\n",
    "\treturn all_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create stacked model input dataset as outputs from the ensemble\n",
    "def stacked_dataset(members, inputX):\n",
    "\tstackX = None\n",
    "\tfor model in members:\n",
    "\t\t# make prediction\n",
    "\t\tyhat = model.predict(inputX, verbose=0)\n",
    "\t\t# stack predictions into [rows, members, probabilities]\n",
    "\t\tif stackX is None:\n",
    "\t\t\tstackX = yhat\n",
    "\t\telse:\n",
    "\t\t\tstackX = dstack((stackX, yhat))\n",
    "\t# flatten predictions to [rows, members x probabilities]\n",
    "\tstackX = stackX.reshape((stackX.shape[0], stackX.shape[1]*stackX.shape[2]))\n",
    "\treturn stackX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit a model based on the outputs from the ensemble members\n",
    "def fit_stacked_model(members, inputX, inputy):\n",
    "\t# create dataset using ensemble\n",
    "\tstackedX = stacked_dataset(members, inputX)\n",
    "\t# fit standalone model\n",
    "\tmodel = LogisticRegression()\n",
    "\tmodel.fit(stackedX, inputy)\n",
    "\treturn model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a prediction with the stacked model\n",
    "def stacked_prediction(members, model, inputX):\n",
    "\t# create dataset using ensemble\n",
    "\tstackedX = stacked_dataset(members, inputX)\n",
    "\t# make a prediction\n",
    "\tyhat = model.predict(stackedX)\n",
    "\treturn yhat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">loaded models/model_1.h5\n",
      ">loaded models/model_2.h5\n",
      ">loaded models/model_3.h5\n",
      ">loaded models/model_4.h5\n",
      ">loaded models/model_5.h5\n",
      "Loaded 5 models\n"
     ]
    }
   ],
   "source": [
    "# load all models\n",
    "n_members = 5\n",
    "members = load_all_models(n_members)\n",
    "print('Loaded %d models' % len(members))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 2) (1000, 2)\n"
     ]
    }
   ],
   "source": [
    "# generate 2d classification dataset\n",
    "X, y = make_blobs(n_samples=1100, centers=3, n_features=2, cluster_std=2, random_state=2)\n",
    "# split into train and test\n",
    "n_train = 100\n",
    "trainX, testX = X[:n_train, :], X[n_train:, :]\n",
    "trainy, testy = y[:n_train], y[n_train:]\n",
    "print(trainX.shape, testX.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Accuracy: 0.817\n",
      "Model Accuracy: 0.810\n",
      "Model Accuracy: 0.807\n",
      "Model Accuracy: 0.813\n",
      "Model Accuracy: 0.807\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'LogisticRegression' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-5430f50296fd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Model Accuracy: %.3f'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0macc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;31m# fit stacked model using the ensemble\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfit_stacked_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmembers\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtestX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtesty\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;31m# evaluate model on test set\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[0myhat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstacked_prediction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmembers\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtestX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-9-40f89624dcce>\u001b[0m in \u001b[0;36mfit_stacked_model\u001b[1;34m(members, inputX, inputy)\u001b[0m\n\u001b[0;32m      4\u001b[0m         \u001b[0mstackedX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstacked_dataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmembers\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[1;31m# fit standalone model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m         \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLogisticRegression\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstackedX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'LogisticRegression' is not defined"
     ]
    }
   ],
   "source": [
    "# evaluate standalone models on test dataset\n",
    "for model in members:\n",
    "\ttesty_enc = to_categorical(testy)\n",
    "\t_, acc = model.evaluate(testX, testy_enc, verbose=0)\n",
    "\tprint('Model Accuracy: %.3f' % acc)\n",
    "# fit stacked model using the ensemble\n",
    "model = fit_stacked_model(members, testX, testy)\n",
    "# evaluate model on test set\n",
    "yhat = stacked_prediction(members, model, testX)\n",
    "acc = accuracy_score(testy, yhat)\n",
    "print('Stacked Test Accuracy: %.3f' % acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define stacked model from multiple member input models\n",
    "def define_stacked_model(members):\n",
    "\t# update all layers in all models to not be trainable\n",
    "\tfor i in range(len(members)):\n",
    "\t\tmodel = members[i]\n",
    "\t\tfor layer in model.layers:\n",
    "\t\t\t# make not trainable\n",
    "\t\t\tlayer.trainable = False\n",
    "\t\t\t# rename to avoid 'unique layer name' issue\n",
    "\t\t\tlayer._name = 'ensemble_' + str(i+1) + '_' + layer.name\n",
    "\t# define multi-headed input\n",
    "\tensemble_visible = [model.input for model in members]\n",
    "\t# concatenate merge output from each model\n",
    "\tensemble_outputs = [model.output for model in members]\n",
    "\tmerge = concatenate(ensemble_outputs)\n",
    "\thidden = Dense(10, activation='relu')(merge)\n",
    "\toutput = Dense(3, activation='softmax')(hidden)\n",
    "\tmodel = Model(inputs=ensemble_visible, outputs=output)\n",
    "\t# plot graph of ensemble\n",
    "\tplot_model(model, show_shapes=True, to_file='model_graph.png')\n",
    "\t# compile\n",
    "\tmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\treturn model\n",
    " \n",
    "# fit a stacked model\n",
    "def fit_stacked_model(model, inputX, inputy):\n",
    "\t# prepare input data\n",
    "\tX = [inputX for _ in range(len(model.input))]\n",
    "\t# encode output data\n",
    "\tinputy_enc = to_categorical(inputy)\n",
    "\t# fit model\n",
    "\tmodel.fit(X, inputy_enc, epochs=300, verbose=0)\n",
    " \n",
    "# make a prediction with a stacked model\n",
    "def predict_stacked_model(model, inputX):\n",
    "\t# prepare input data\n",
    "\tX = [inputX for _ in range(len(model.input))]\n",
    "\t# make prediction\n",
    "\treturn model.predict(X, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# generate 2d classification dataset\n",
    "X, y = make_blobs(n_samples=1100, centers=3, n_features=2, cluster_std=2, random_state=2)\n",
    "# split into train and test\n",
    "n_train = 100\n",
    "trainX, testX = X[:n_train, :], X[n_train:, :]\n",
    "trainy, testy = y[:n_train], y[n_train:]\n",
    "print(trainX.shape, testX.shape)\n",
    "# load all models\n",
    "n_members = 5\n",
    "members = load_all_models(n_members)\n",
    "print('Loaded %d models' % len(members))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Integrated Stacking Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.metrics import accuracy_score\n",
    "from keras.models import load_model\n",
    "from keras.utils import to_categorical\n",
    "from keras.utils import plot_model\n",
    "from keras.models import Model\n",
    "from keras.layers import Input\n",
    "from keras.layers import Dense\n",
    "from keras.layers.merge import concatenate\n",
    "from numpy import argmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load models from file\n",
    "def load_all_models(n_models):\n",
    "\tall_models = list()\n",
    "\tfor i in range(n_models):\n",
    "\t\t# define filename for this ensemble\n",
    "\t\tfilename = 'models/model_' + str(i + 1) + '.h5'\n",
    "\t\t# load model from file\n",
    "\t\tmodel = load_model(filename)\n",
    "\t\t# add to list of members\n",
    "\t\tall_models.append(model)\n",
    "\t\tprint('>loaded %s' % filename)\n",
    "\treturn all_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define stacked model from multiple member input models\n",
    "def define_stacked_model(members):\n",
    "\t# update all layers in all models to not be trainable\n",
    "\tfor i in range(len(members)):\n",
    "\t\tmodel = members[i]\n",
    "\t\tfor layer in model.layers:\n",
    "\t\t\t# make not trainable\n",
    "\t\t\tlayer.trainable = False\n",
    "\t\t\t# rename to avoid 'unique layer name' issue\n",
    "\t\t\tlayer._name = 'ensemble_' + str(i+1) + '_' + layer.name\n",
    "\t# define multi-headed input\n",
    "\tensemble_visible = [model.input for model in members]\n",
    "\t# concatenate merge output from each model\n",
    "\tensemble_outputs = [model.output for model in members]\n",
    "\tmerge = concatenate(ensemble_outputs)\n",
    "\thidden = Dense(10, activation='relu')(merge)\n",
    "\toutput = Dense(3, activation='softmax')(hidden)\n",
    "\tmodel = Model(inputs=ensemble_visible, outputs=output)\n",
    "\t# plot graph of ensemble\n",
    "#\tplot_model(model, show_shapes=True, to_file='model_graph.png')\n",
    "\t# compile\n",
    "\tmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\treturn model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit a stacked model\n",
    "def fit_stacked_model(model, inputX, inputy):\n",
    "\t# prepare input data\n",
    "\tX = [inputX for _ in range(len(model.input))]\n",
    "\t# encode output data\n",
    "\tinputy_enc = to_categorical(inputy)\n",
    "\t# fit model\n",
    "\tmodel.fit(X, inputy_enc, epochs=300, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a prediction with a stacked model\n",
    "def predict_stacked_model(model, inputX):\n",
    "\t# prepare input data\n",
    "\tX = [inputX for _ in range(len(model.input))]\n",
    "\t# make prediction\n",
    "\treturn model.predict(X, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate 2d classification dataset\n",
    "X, y = make_blobs(n_samples=1100, centers=3, n_features=2, cluster_std=2, random_state=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into train and test\n",
    "n_train = 100\n",
    "trainX, testX = X[:n_train, :], X[n_train:, :]\n",
    "trainy, testy = y[:n_train], y[n_train:]\n",
    "print(trainX.shape, testX.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load all models\n",
    "n_members = 5\n",
    "members = load_all_models(n_members)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Loaded %d models' % len(members))\n",
    "# define ensemble model\n",
    "stacked_model = define_stacked_model(members)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit stacked model on test dataset\n",
    "fit_stacked_model(stacked_model, testX, testy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make predictions and evaluate\n",
    "yhat = predict_stacked_model(stacked_model, testX)\n",
    "yhat = argmax(yhat, axis=1)\n",
    "acc = accuracy_score(testy, yhat)\n",
    "print('Stacked Test Accuracy: %.3f' % acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM and CNN for sequence classification in the IMDB dataset\n",
    "import numpy\n",
    "from keras.datasets import imdb\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    "layers.concatenate\n",
    "from keras.preprocessing import sequence\n",
    "# fix random seed for reproducibility\n",
    "numpy.random.seed(7)\n",
    "# load the dataset but only keep the top n words, zero the rest\n",
    "top_words = 5000\n",
    "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=top_words)\n",
    "# truncate and pad input sequences\n",
    "max_review_length = 500\n",
    "X_train = sequence.pad_sequences(X_train, maxlen=max_review_length)\n",
    "X_test = sequence.pad_sequences(X_test, maxlen=max_review_length)\n",
    "# create the model\n",
    "embedding_vecor_length = 32\n",
    "model = Sequential()\n",
    "model.add(Embedding(top_words, embedding_vecor_length, input_length=max_review_length))\n",
    "model.add(Conv1D(filters=32, kernel_size=3, padding='same', activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(LSTM(100))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())\n",
    "model.fit(X_train, y_train, epochs=3, batch_size=64)\n",
    "# Final evaluation of the model\n",
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = [] #  empty regular list\n",
    "for i in range(len(dataframe1)):\n",
    "    arr.append(1*np.ones((1)))\n",
    "for i in range(len(dataframe2)):\n",
    "    arr.append(2*np.ones((1)))\n",
    "for i in range(len(dataframe3)):\n",
    "    arr.append(3*np.ones((1)))\n",
    "for i in range(len(dataframe4)):\n",
    "    arr.append(4*np.ones((1)))\n",
    "np_array = np.array(arr)  # transformed to a numpy array\n",
    "\n",
    "da = pd.DataFrame({'': np_array[:, 0]})\n",
    "print(da)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from attention_keras.layers.attention import AttentionLayer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unable to create process using 'C:\\Users\\Hyunju Kim\\.conda\\envs\\tf2.0-gpu\\python.exe \"C:\\Users\\Hyunju Kim\\.conda\\envs\\tf2.0-gpu\\Scripts\\pip-script.py\" install attention'\n"
     ]
    }
   ],
   "source": [
    "!pip install attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 2) (1000, 2)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Unknown initializer: GlorotUniform",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-673e12786332>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     61\u001b[0m \u001b[1;31m# load all models\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[0mn_members\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m5\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m \u001b[0mmembers\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_all_models\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_members\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Loaded %d models'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmembers\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[1;31m# evaluate standalone models on test dataset\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-5-673e12786332>\u001b[0m in \u001b[0;36mload_all_models\u001b[1;34m(n_models)\u001b[0m\n\u001b[0;32m     14\u001b[0m                 \u001b[0mfilename\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'models/model_'\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'.h5'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m                 \u001b[1;31m# load model from file\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m                 \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m                 \u001b[1;31m# add to list of members\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m                 \u001b[0mall_models\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Anaconda3\\envs\\tf2.0-gpu\\lib\\site-packages\\keras\\engine\\saving.py\u001b[0m in \u001b[0;36mload_wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    490\u001b[0m                 \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mremove\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtmp_filepath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    491\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mres\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 492\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mload_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    493\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    494\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mload_wrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Anaconda3\\envs\\tf2.0-gpu\\lib\\site-packages\\keras\\engine\\saving.py\u001b[0m in \u001b[0;36mload_model\u001b[1;34m(filepath, custom_objects, compile)\u001b[0m\n\u001b[0;32m    582\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mH5Dict\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_supported_type\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    583\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mH5Dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'r'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mh5dict\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 584\u001b[1;33m             \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_deserialize_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mh5dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcustom_objects\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcompile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    585\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'write'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mcallable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    586\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mload_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mh5file\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Anaconda3\\envs\\tf2.0-gpu\\lib\\site-packages\\keras\\engine\\saving.py\u001b[0m in \u001b[0;36m_deserialize_model\u001b[1;34m(h5dict, custom_objects, compile)\u001b[0m\n\u001b[0;32m    272\u001b[0m         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'No model found in config.'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    273\u001b[0m     \u001b[0mmodel_config\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_config\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'utf-8'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 274\u001b[1;33m     \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel_from_config\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_config\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcustom_objects\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcustom_objects\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    275\u001b[0m     \u001b[0mmodel_weights_group\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mh5dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'model_weights'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    276\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Anaconda3\\envs\\tf2.0-gpu\\lib\\site-packages\\keras\\engine\\saving.py\u001b[0m in \u001b[0;36mmodel_from_config\u001b[1;34m(config, custom_objects)\u001b[0m\n\u001b[0;32m    625\u001b[0m                         '`Sequential.from_config(config)`?')\n\u001b[0;32m    626\u001b[0m     \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdeserialize\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 627\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mdeserialize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcustom_objects\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcustom_objects\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    628\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    629\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Anaconda3\\envs\\tf2.0-gpu\\lib\\site-packages\\keras\\layers\\__init__.py\u001b[0m in \u001b[0;36mdeserialize\u001b[1;34m(config, custom_objects)\u001b[0m\n\u001b[0;32m    166\u001b[0m                                     \u001b[0mmodule_objects\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mglobs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    167\u001b[0m                                     \u001b[0mcustom_objects\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcustom_objects\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 168\u001b[1;33m                                     printable_module_name='layer')\n\u001b[0m",
      "\u001b[1;32mC:\\Users\\Anaconda3\\envs\\tf2.0-gpu\\lib\\site-packages\\keras\\utils\\generic_utils.py\u001b[0m in \u001b[0;36mdeserialize_keras_object\u001b[1;34m(identifier, module_objects, custom_objects, printable_module_name)\u001b[0m\n\u001b[0;32m    145\u001b[0m                     \u001b[0mconfig\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'config'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    146\u001b[0m                     custom_objects=dict(list(_GLOBAL_CUSTOM_OBJECTS.items()) +\n\u001b[1;32m--> 147\u001b[1;33m                                         list(custom_objects.items())))\n\u001b[0m\u001b[0;32m    148\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mCustomObjectScope\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcustom_objects\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    149\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_config\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'config'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Anaconda3\\envs\\tf2.0-gpu\\lib\\site-packages\\keras\\engine\\sequential.py\u001b[0m in \u001b[0;36mfrom_config\u001b[1;34m(cls, config, custom_objects)\u001b[0m\n\u001b[0;32m    299\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mconf\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mlayer_configs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    300\u001b[0m             layer = layer_module.deserialize(conf,\n\u001b[1;32m--> 301\u001b[1;33m                                              custom_objects=custom_objects)\n\u001b[0m\u001b[0;32m    302\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    303\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minputs\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mbuild_input_shape\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Anaconda3\\envs\\tf2.0-gpu\\lib\\site-packages\\keras\\layers\\__init__.py\u001b[0m in \u001b[0;36mdeserialize\u001b[1;34m(config, custom_objects)\u001b[0m\n\u001b[0;32m    166\u001b[0m                                     \u001b[0mmodule_objects\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mglobs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    167\u001b[0m                                     \u001b[0mcustom_objects\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcustom_objects\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 168\u001b[1;33m                                     printable_module_name='layer')\n\u001b[0m",
      "\u001b[1;32mC:\\Users\\Anaconda3\\envs\\tf2.0-gpu\\lib\\site-packages\\keras\\utils\\generic_utils.py\u001b[0m in \u001b[0;36mdeserialize_keras_object\u001b[1;34m(identifier, module_objects, custom_objects, printable_module_name)\u001b[0m\n\u001b[0;32m    147\u001b[0m                                         list(custom_objects.items())))\n\u001b[0;32m    148\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mCustomObjectScope\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcustom_objects\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 149\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_config\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'config'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    150\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    151\u001b[0m             \u001b[1;31m# Then `cls` may be a function returning a class.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Anaconda3\\envs\\tf2.0-gpu\\lib\\site-packages\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36mfrom_config\u001b[1;34m(cls, config)\u001b[0m\n\u001b[0;32m   1177\u001b[0m             \u001b[0mA\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0minstance\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1178\u001b[0m         \"\"\"\n\u001b[1;32m-> 1179\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1180\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1181\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mcount_params\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Anaconda3\\envs\\tf2.0-gpu\\lib\\site-packages\\keras\\legacy\\interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[0;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[1;32m---> 91\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Anaconda3\\envs\\tf2.0-gpu\\lib\\site-packages\\keras\\layers\\core.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, units, activation, use_bias, kernel_initializer, bias_initializer, kernel_regularizer, bias_regularizer, activity_regularizer, kernel_constraint, bias_constraint, **kwargs)\u001b[0m\n\u001b[0;32m    875\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mactivation\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mactivations\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mactivation\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    876\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muse_bias\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0muse_bias\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 877\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkernel_initializer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minitializers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkernel_initializer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    878\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias_initializer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minitializers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbias_initializer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    879\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkernel_regularizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mregularizers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkernel_regularizer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Anaconda3\\envs\\tf2.0-gpu\\lib\\site-packages\\keras\\initializers.py\u001b[0m in \u001b[0;36mget\u001b[1;34m(identifier)\u001b[0m\n\u001b[0;32m    513\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0midentifier\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    514\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0midentifier\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 515\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mdeserialize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0midentifier\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    516\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0midentifier\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstring_types\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    517\u001b[0m         \u001b[0mconfig\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m'class_name'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0midentifier\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'config'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Anaconda3\\envs\\tf2.0-gpu\\lib\\site-packages\\keras\\initializers.py\u001b[0m in \u001b[0;36mdeserialize\u001b[1;34m(config, custom_objects)\u001b[0m\n\u001b[0;32m    508\u001b[0m                                     \u001b[0mmodule_objects\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mglobals\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    509\u001b[0m                                     \u001b[0mcustom_objects\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcustom_objects\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 510\u001b[1;33m                                     printable_module_name='initializer')\n\u001b[0m\u001b[0;32m    511\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    512\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Anaconda3\\envs\\tf2.0-gpu\\lib\\site-packages\\keras\\utils\\generic_utils.py\u001b[0m in \u001b[0;36mdeserialize_keras_object\u001b[1;34m(identifier, module_objects, custom_objects, printable_module_name)\u001b[0m\n\u001b[0;32m    138\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mcls\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    139\u001b[0m                 raise ValueError('Unknown ' + printable_module_name +\n\u001b[1;32m--> 140\u001b[1;33m                                  ': ' + class_name)\n\u001b[0m\u001b[0;32m    141\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'from_config'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    142\u001b[0m             \u001b[0mcustom_objects\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcustom_objects\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Unknown initializer: GlorotUniform"
     ]
    }
   ],
   "source": [
    "# stacked generalization with linear meta model on blobs dataset\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from keras.models import load_model\n",
    "from keras.utils import to_categorical\n",
    "from numpy import dstack\n",
    " \n",
    "# load models from file\n",
    "def load_all_models(n_models):\n",
    "\tall_models = list()\n",
    "\tfor i in range(n_models):\n",
    "\t\t# define filename for this ensemble\n",
    "\t\tfilename = 'models/model_' + str(i + 1) + '.h5'\n",
    "\t\t# load model from file\n",
    "\t\tmodel = load_model(filename)\n",
    "\t\t# add to list of members\n",
    "\t\tall_models.append(model)\n",
    "\t\tprint('>loaded %s' % filename)\n",
    "\treturn all_models\n",
    " \n",
    "# create stacked model input dataset as outputs from the ensemble\n",
    "def stacked_dataset(members, inputX):\n",
    "\tstackX = None\n",
    "\tfor model in members:\n",
    "\t\t# make prediction\n",
    "\t\tyhat = model.predict(inputX, verbose=0)\n",
    "\t\t# stack predictions into [rows, members, probabilities]\n",
    "\t\tif stackX is None:\n",
    "\t\t\tstackX = yhat\n",
    "\t\telse:\n",
    "\t\t\tstackX = dstack((stackX, yhat))\n",
    "\t# flatten predictions to [rows, members x probabilities]\n",
    "\tstackX = stackX.reshape((stackX.shape[0], stackX.shape[1]*stackX.shape[2]))\n",
    "\treturn stackX\n",
    " \n",
    "# fit a model based on the outputs from the ensemble members\n",
    "def fit_stacked_model(members, inputX, inputy):\n",
    "\t# create dataset using ensemble\n",
    "\tstackedX = stacked_dataset(members, inputX)\n",
    "\t# fit standalone model\n",
    "\tmodel = LogisticRegression()\n",
    "\tmodel.fit(stackedX, inputy)\n",
    "\treturn model\n",
    " \n",
    "# make a prediction with the stacked model\n",
    "def stacked_prediction(members, model, inputX):\n",
    "\t# create dataset using ensemble\n",
    "\tstackedX = stacked_dataset(members, inputX)\n",
    "\t# make a prediction\n",
    "\tyhat = model.predict(stackedX)\n",
    "\treturn yhat\n",
    " \n",
    "# generate 2d classification dataset\n",
    "X, y = make_blobs(n_samples=1100, centers=3, n_features=2, cluster_std=2, random_state=2)\n",
    "# split into train and test\n",
    "n_train = 100\n",
    "trainX, testX = X[:n_train, :], X[n_train:, :]\n",
    "trainy, testy = y[:n_train], y[n_train:]\n",
    "print(trainX.shape, testX.shape)\n",
    "# load all models\n",
    "n_members = 5\n",
    "members = load_all_models(n_members)\n",
    "print('Loaded %d models' % len(members))\n",
    "# evaluate standalone models on test dataset\n",
    "for model in members:\n",
    "\ttesty_enc = to_categorical(testy)\n",
    "\t_, acc = model.evaluate(testX, testy_enc, verbose=0)\n",
    "\tprint('Model Accuracy: %.3f' % acc)\n",
    "# fit stacked model using the ensemble\n",
    "model = fit_stacked_model(members, testX, testy)\n",
    "print(testX.shape)\n",
    "# evaluate model on test set\n",
    "yhat = stacked_prediction(members, model, testX)\n",
    "acc = accuracy_score(testy, yhat)\n",
    "print('Stacked Test Accuracy: %.3f' % acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 2) (1000, 2)\n"
     ]
    }
   ],
   "source": [
    "# generate 2d classification dataset\n",
    "X, y = make_blobs(n_samples=1100, centers=3, n_features=2, cluster_std=2, random_state=2)\n",
    "# split into train and test\n",
    "n_train = 100\n",
    "trainX, testX = X[:n_train, :], X[n_train:, :]\n",
    "trainy, testy = y[:n_train], y[n_train:]\n",
    "print(trainX.shape, testX.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 0 2 ... 1 0 2]\n"
     ]
    }
   ],
   "source": [
    "# generate 2d classification dataset\n",
    "X, y = make_blobs(n_samples=1100, centers=3, n_features=2, cluster_std=2, random_state=2)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 2) (1000, 2)\n",
      "(100,) (1000,)\n"
     ]
    }
   ],
   "source": [
    "n_train = 100\n",
    "trainX, testX = X[:n_train, :], X[n_train:, :]\n",
    "trainy, testy = y[:n_train], y[n_train:]\n",
    "print(trainX.shape, testX.shape)\n",
    "print(trainy.shape, testy.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 2) (1000, 2)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Unknown initializer: GlorotUniform",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-85495832bacc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     61\u001b[0m \u001b[1;31m# load all models\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[0mn_members\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m5\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m \u001b[0mmembers\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_all_models\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_members\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Loaded %d models'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmembers\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[1;31m# evaluate standalone models on test dataset\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-12-85495832bacc>\u001b[0m in \u001b[0;36mload_all_models\u001b[1;34m(n_models)\u001b[0m\n\u001b[0;32m     14\u001b[0m                 \u001b[0mfilename\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'models/model_'\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'.h5'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m                 \u001b[1;31m# load model from file\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m                 \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m                 \u001b[1;31m# add to list of members\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m                 \u001b[0mall_models\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Anaconda3\\envs\\tf2.0-gpu\\lib\\site-packages\\keras\\engine\\saving.py\u001b[0m in \u001b[0;36mload_wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    490\u001b[0m                 \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mremove\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtmp_filepath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    491\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mres\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 492\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mload_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    493\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    494\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mload_wrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Anaconda3\\envs\\tf2.0-gpu\\lib\\site-packages\\keras\\engine\\saving.py\u001b[0m in \u001b[0;36mload_model\u001b[1;34m(filepath, custom_objects, compile)\u001b[0m\n\u001b[0;32m    582\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mH5Dict\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_supported_type\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    583\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mH5Dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'r'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mh5dict\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 584\u001b[1;33m             \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_deserialize_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mh5dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcustom_objects\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcompile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    585\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'write'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mcallable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    586\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mload_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mh5file\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Anaconda3\\envs\\tf2.0-gpu\\lib\\site-packages\\keras\\engine\\saving.py\u001b[0m in \u001b[0;36m_deserialize_model\u001b[1;34m(h5dict, custom_objects, compile)\u001b[0m\n\u001b[0;32m    272\u001b[0m         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'No model found in config.'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    273\u001b[0m     \u001b[0mmodel_config\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_config\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'utf-8'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 274\u001b[1;33m     \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel_from_config\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_config\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcustom_objects\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcustom_objects\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    275\u001b[0m     \u001b[0mmodel_weights_group\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mh5dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'model_weights'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    276\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Anaconda3\\envs\\tf2.0-gpu\\lib\\site-packages\\keras\\engine\\saving.py\u001b[0m in \u001b[0;36mmodel_from_config\u001b[1;34m(config, custom_objects)\u001b[0m\n\u001b[0;32m    625\u001b[0m                         '`Sequential.from_config(config)`?')\n\u001b[0;32m    626\u001b[0m     \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdeserialize\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 627\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mdeserialize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcustom_objects\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcustom_objects\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    628\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    629\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Anaconda3\\envs\\tf2.0-gpu\\lib\\site-packages\\keras\\layers\\__init__.py\u001b[0m in \u001b[0;36mdeserialize\u001b[1;34m(config, custom_objects)\u001b[0m\n\u001b[0;32m    166\u001b[0m                                     \u001b[0mmodule_objects\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mglobs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    167\u001b[0m                                     \u001b[0mcustom_objects\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcustom_objects\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 168\u001b[1;33m                                     printable_module_name='layer')\n\u001b[0m",
      "\u001b[1;32mC:\\Users\\Anaconda3\\envs\\tf2.0-gpu\\lib\\site-packages\\keras\\utils\\generic_utils.py\u001b[0m in \u001b[0;36mdeserialize_keras_object\u001b[1;34m(identifier, module_objects, custom_objects, printable_module_name)\u001b[0m\n\u001b[0;32m    145\u001b[0m                     \u001b[0mconfig\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'config'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    146\u001b[0m                     custom_objects=dict(list(_GLOBAL_CUSTOM_OBJECTS.items()) +\n\u001b[1;32m--> 147\u001b[1;33m                                         list(custom_objects.items())))\n\u001b[0m\u001b[0;32m    148\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mCustomObjectScope\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcustom_objects\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    149\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_config\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'config'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Anaconda3\\envs\\tf2.0-gpu\\lib\\site-packages\\keras\\engine\\sequential.py\u001b[0m in \u001b[0;36mfrom_config\u001b[1;34m(cls, config, custom_objects)\u001b[0m\n\u001b[0;32m    299\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mconf\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mlayer_configs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    300\u001b[0m             layer = layer_module.deserialize(conf,\n\u001b[1;32m--> 301\u001b[1;33m                                              custom_objects=custom_objects)\n\u001b[0m\u001b[0;32m    302\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    303\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minputs\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mbuild_input_shape\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Anaconda3\\envs\\tf2.0-gpu\\lib\\site-packages\\keras\\layers\\__init__.py\u001b[0m in \u001b[0;36mdeserialize\u001b[1;34m(config, custom_objects)\u001b[0m\n\u001b[0;32m    166\u001b[0m                                     \u001b[0mmodule_objects\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mglobs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    167\u001b[0m                                     \u001b[0mcustom_objects\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcustom_objects\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 168\u001b[1;33m                                     printable_module_name='layer')\n\u001b[0m",
      "\u001b[1;32mC:\\Users\\Anaconda3\\envs\\tf2.0-gpu\\lib\\site-packages\\keras\\utils\\generic_utils.py\u001b[0m in \u001b[0;36mdeserialize_keras_object\u001b[1;34m(identifier, module_objects, custom_objects, printable_module_name)\u001b[0m\n\u001b[0;32m    147\u001b[0m                                         list(custom_objects.items())))\n\u001b[0;32m    148\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mCustomObjectScope\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcustom_objects\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 149\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_config\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'config'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    150\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    151\u001b[0m             \u001b[1;31m# Then `cls` may be a function returning a class.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Anaconda3\\envs\\tf2.0-gpu\\lib\\site-packages\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36mfrom_config\u001b[1;34m(cls, config)\u001b[0m\n\u001b[0;32m   1177\u001b[0m             \u001b[0mA\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0minstance\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1178\u001b[0m         \"\"\"\n\u001b[1;32m-> 1179\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1180\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1181\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mcount_params\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Anaconda3\\envs\\tf2.0-gpu\\lib\\site-packages\\keras\\legacy\\interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[0;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[1;32m---> 91\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Anaconda3\\envs\\tf2.0-gpu\\lib\\site-packages\\keras\\layers\\core.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, units, activation, use_bias, kernel_initializer, bias_initializer, kernel_regularizer, bias_regularizer, activity_regularizer, kernel_constraint, bias_constraint, **kwargs)\u001b[0m\n\u001b[0;32m    875\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mactivation\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mactivations\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mactivation\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    876\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muse_bias\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0muse_bias\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 877\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkernel_initializer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minitializers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkernel_initializer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    878\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias_initializer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minitializers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbias_initializer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    879\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkernel_regularizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mregularizers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkernel_regularizer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Anaconda3\\envs\\tf2.0-gpu\\lib\\site-packages\\keras\\initializers.py\u001b[0m in \u001b[0;36mget\u001b[1;34m(identifier)\u001b[0m\n\u001b[0;32m    513\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0midentifier\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    514\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0midentifier\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 515\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mdeserialize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0midentifier\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    516\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0midentifier\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstring_types\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    517\u001b[0m         \u001b[0mconfig\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m'class_name'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0midentifier\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'config'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Anaconda3\\envs\\tf2.0-gpu\\lib\\site-packages\\keras\\initializers.py\u001b[0m in \u001b[0;36mdeserialize\u001b[1;34m(config, custom_objects)\u001b[0m\n\u001b[0;32m    508\u001b[0m                                     \u001b[0mmodule_objects\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mglobals\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    509\u001b[0m                                     \u001b[0mcustom_objects\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcustom_objects\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 510\u001b[1;33m                                     printable_module_name='initializer')\n\u001b[0m\u001b[0;32m    511\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    512\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Anaconda3\\envs\\tf2.0-gpu\\lib\\site-packages\\keras\\utils\\generic_utils.py\u001b[0m in \u001b[0;36mdeserialize_keras_object\u001b[1;34m(identifier, module_objects, custom_objects, printable_module_name)\u001b[0m\n\u001b[0;32m    138\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mcls\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    139\u001b[0m                 raise ValueError('Unknown ' + printable_module_name +\n\u001b[1;32m--> 140\u001b[1;33m                                  ': ' + class_name)\n\u001b[0m\u001b[0;32m    141\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'from_config'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    142\u001b[0m             \u001b[0mcustom_objects\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcustom_objects\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Unknown initializer: GlorotUniform"
     ]
    }
   ],
   "source": [
    "\n",
    "# stacked generalization with linear meta model on blobs dataset\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from keras.models import load_model\n",
    "from keras.utils import to_categorical\n",
    "from numpy import dstack\n",
    "\n",
    "# load models from file\n",
    "def load_all_models(n_models):\n",
    "\tall_models = list()\n",
    "\tfor i in range(n_models):\n",
    "\t\t# define filename for this ensemble\n",
    "\t\tfilename = 'models/model_' + str(i + 1) + '.h5'\n",
    "\t\t# load model from file\n",
    "\t\tmodel = load_model(filename)\n",
    "\t\t# add to list of members\n",
    "\t\tall_models.append(model)\n",
    "\t\tprint('>loaded %s' % filename)\n",
    "\treturn all_models\n",
    "\n",
    "# create stacked model input dataset as outputs from the ensemble\n",
    "def stacked_dataset(members, inputX):\n",
    "\tstackX = None\n",
    "\tfor model in members:\n",
    "\t\t# make prediction\n",
    "\t\tyhat = model.predict(inputX, verbose=0)\n",
    "\t\t# stack predictions into [rows, members, probabilities]\n",
    "\t\tif stackX is None:\n",
    "\t\t\tstackX = yhat\n",
    "\t\telse:\n",
    "\t\t\tstackX = dstack((stackX, yhat))\n",
    "\t# flatten predictions to [rows, members x probabilities]\n",
    "\tstackX = stackX.reshape((stackX.shape[0], stackX.shape[1]*stackX.shape[2]))\n",
    "\treturn stackX\n",
    "\n",
    "# fit a model based on the outputs from the ensemble members\n",
    "def fit_stacked_model(members, inputX, inputy):\n",
    "\t# create dataset using ensemble\n",
    "\tstackedX = stacked_dataset(members, inputX)\n",
    "\t# fit standalone model\n",
    "\tmodel = LogisticRegression()\n",
    "\tmodel.fit(stackedX, inputy)\n",
    "\treturn model\n",
    "\n",
    "# make a prediction with the stacked model\n",
    "def stacked_prediction(members, model, inputX):\n",
    "\t# create dataset using ensemble\n",
    "\tstackedX = stacked_dataset(members, inputX)\n",
    "\t# make a prediction\n",
    "\tyhat = model.predict(stackedX)\n",
    "\treturn yhat\n",
    "\n",
    "# generate 2d classification dataset\n",
    "X, y = make_blobs(n_samples=1100, centers=3, n_features=2, cluster_std=2, random_state=2)\n",
    "# split into train and test\n",
    "n_train = 100\n",
    "trainX, testX = X[:n_train, :], X[n_train:, :]\n",
    "trainy, testy = y[:n_train], y[n_train:]\n",
    "print(trainX.shape, testX.shape)\n",
    "# load all models\n",
    "n_members = 5\n",
    "members = load_all_models(n_members)\n",
    "print('Loaded %d models' % len(members))\n",
    "# evaluate standalone models on test dataset\n",
    "for model in members:\n",
    "\ttesty_enc = to_categorical(testy)\n",
    "\t_, acc = model.evaluate(testX, testy_enc, verbose=0)\n",
    "\tprint('Model Accuracy: %.3f' % acc)\n",
    "# fit stacked model using the ensemble\n",
    "model = fit_stacked_model(members, testX, testy)\n",
    "# evaluate model on test set\n",
    "yhat = stacked_prediction(members, model, testX)\n",
    "acc = accuracy_score(testy, yhat)\n",
    "print('Stacked Test Accuracy: %.3f' % acc)\n",
    "\n",
    "# stacked generalization with linear meta model on blobs dataset\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from keras.models import load_model\n",
    "from keras.utils import to_categorical\n",
    "from numpy import dstack\n",
    " \n",
    "# load models from file\n",
    "def load_all_models(n_models):\n",
    "\tall_models = list()\n",
    "\tfor i in range(n_models):\n",
    "\t\t# define filename for this ensemble\n",
    "\t\tfilename = 'models/model_' + str(i + 1) + '.h5'\n",
    "\t\t# load model from file\n",
    "\t\tmodel = load_model(filename)\n",
    "\t\t# add to list of members\n",
    "\t\tall_models.append(model)\n",
    "\t\tprint('>loaded %s' % filename)\n",
    "\treturn all_models\n",
    " \n",
    "# create stacked model input dataset as outputs from the ensemble\n",
    "def stacked_dataset(members, inputX):\n",
    "\tstackX = None\n",
    "\tfor model in members:\n",
    "\t\t# make prediction\n",
    "\t\tyhat = model.predict(inputX, verbose=0)\n",
    "\t\t# stack predictions into [rows, members, probabilities]\n",
    "\t\tif stackX is None:\n",
    "\t\t\tstackX = yhat\n",
    "\t\telse:\n",
    "\t\t\tstackX = dstack((stackX, yhat))\n",
    "\t# flatten predictions to [rows, members x probabilities]\n",
    "\tstackX = stackX.reshape((stackX.shape[0], stackX.shape[1]*stackX.shape[2]))\n",
    "\treturn stackX\n",
    " \n",
    "# fit a model based on the outputs from the ensemble members\n",
    "def fit_stacked_model(members, inputX, inputy):\n",
    "\t# create dataset using ensemble\n",
    "\tstackedX = stacked_dataset(members, inputX)\n",
    "\t# fit standalone model\n",
    "\tmodel = LogisticRegression()\n",
    "\tmodel.fit(stackedX, inputy)\n",
    "\treturn model\n",
    " \n",
    "# make a prediction with the stacked model\n",
    "def stacked_prediction(members, model, inputX):\n",
    "\t# create dataset using ensemble\n",
    "\tstackedX = stacked_dataset(members, inputX)\n",
    "\t# make a prediction\n",
    "\tyhat = model.predict(stackedX)\n",
    "\treturn yhat\n",
    " \n",
    "# generate 2d classification dataset\n",
    "X, y = make_blobs(n_samples=1100, centers=3, n_features=2, cluster_std=2, random_state=2)\n",
    "# split into train and test\n",
    "n_train = 100\n",
    "trainX, testX = X[:n_train, :], X[n_train:, :]\n",
    "trainy, testy = y[:n_train], y[n_train:]\n",
    "print(trainX.shape, testX.shape)\n",
    "# load all models\n",
    "n_members = 5\n",
    "members = load_all_models(n_members)\n",
    "print('Loaded %d models' % len(members))\n",
    "# evaluate standalone models on test dataset\n",
    "for model in members:\n",
    "\ttesty_enc = to_categorical(testy)\n",
    "\t_, acc = model.evaluate(testX, testy_enc, verbose=0)\n",
    "\tprint('Model Accuracy: %.3f' % acc)\n",
    "# fit stacked model using the ensemble\n",
    "model = fit_stacked_model(members, testX, testy)\n",
    "# evaluate model on test set\n",
    "yhat = stacked_prediction(members, model, testX)\n",
    "acc = accuracy_score(testy, yhat)\n",
    "print('Stacked Test Accuracy: %.3f' % acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 504x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "import numpy as np \n",
    "  \n",
    "  \n",
    "# Creating dataset \n",
    "data_to_plot = [[85.2,88.2,92.6,89.7,91.1,94.1,89.7,88.2,94.1,94.1],\n",
    "                [35.2,29.4,39.7,30.8,39.7,27.9,39.7,38.2,42.6,32.3],\n",
    "                [35.2,29.4,39.7,30.8,30.8,27.9,39.7,38.2,42.6,32.3],\n",
    "                [85.2,94.1,89.7,91.1,82.3,94.1,88.2,88.2,89.7,88.2],\n",
    "                [60.2,63.2,57.3,67.6,52.9,69.1,60.2,47,61.7,50],\n",
    "                [55.8,61.7,57.3,64.7,55.8,61.7,60.2,47,64.7,52.9],\n",
    "                [57.3,55.8,57.3,58.8,50,61.7,50,51.4,52.9,50],\n",
    "                [85.2,91.1,92.6,92.6,91.1,94.1,89.7,88.2,94.1,94.1],\n",
    "                [73.5,76.4,83.8,80.8,77.9,88.2,83.8,76.4,80.8,79.4],\n",
    "                [66.1,61.7,69.1,70.5,58.8,72,67.6,64.7,72,67.6],\n",
    "                [88.2,95.6,95.6,95.6,91.2,98.5,91.2,92.6,94.1,94.1],\n",
    "                [94.1,92.6,83.8,92.6,95.5,95.5,92.6,97,97,98.5]\n",
    "                ]\n",
    "  \n",
    "fig = plt.figure(figsize =(7, 5)) \n",
    "  \n",
    "# Creating plot \n",
    "plt.boxplot(data_to_plot ) \n",
    "  \n",
    "# show plot \n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "import numpy as np \n",
    "  \n",
    "  \n",
    "# Creating dataset \n",
    "data_to_plot = [[92.6,89.7,],\n",
    "                [35.2,29.4,39.7,30.8,39.7,27.9,39.7,38.2,42.6,32.3],\n",
    "                [35.2,29.4,39.7,30.8,30.8,27.9,39.7,38.2,42.6,32.3],\n",
    "                [85.2,94.1,89.7,91.1,82.3,94.1,88.2,88.2,89.7,88.2],\n",
    "                [60.2,63.2,57.3,67.6,52.9,69.1,60.2,47,61.7,50],\n",
    "                [55.8,61.7,57.3,64.7,55.8,61.7,60.2,47,64.7,52.9],\n",
    "                [57.3,55.8,57.3,58.8,50,61.7,50,51.4,52.9,50],\n",
    "                [85.2,91.1,92.6,92.6,91.1,94.1,89.7,88.2,94.1,94.1],\n",
    "                [73.5,76.4,83.8,80.8,77.9,88.2,83.8,76.4,80.8,79.4],\n",
    "                [66.1,61.7,69.1,70.5,58.8,72,67.6,64.7,72,67.6],\n",
    "                [88.2,95.6,95.6,95.6,91.2,98.5,91.2,92.6,94.1,94.1],\n",
    "                [94.1,92.6,83.8,92.6,95.5,95.5,92.6,97,97,98.5]\n",
    "                ]\n",
    "  \n",
    "fig = plt.figure(figsize =(7, 5)) \n",
    "  \n",
    "# Creating plot \n",
    "plt.boxplot(data_to_plot ) \n",
    "  \n",
    "# show plot \n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import numpy as np\n",
    "import pylab as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq = keras.Sequential(\n",
    "    [\n",
    "        keras.Input(\n",
    "            shape=(None, 40, 40, 1)\n",
    "        ),  # Variable-length sequence of 40x40x1 frames\n",
    "        layers.ConvLSTM2D(\n",
    "            filters=40, kernel_size=(3, 3), padding=\"same\", return_sequences=True\n",
    "        ),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.ConvLSTM2D(\n",
    "            filters=40, kernel_size=(3, 3), padding=\"same\", return_sequences=True\n",
    "        ),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.ConvLSTM2D(\n",
    "            filters=40, kernel_size=(3, 3), padding=\"same\", return_sequences=True\n",
    "        ),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.ConvLSTM2D(\n",
    "            filters=40, kernel_size=(3, 3), padding=\"same\", return_sequences=True\n",
    "        ),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Conv3D(\n",
    "            filters=1, kernel_size=(3, 3, 3), activation=\"sigmoid\", padding=\"same\"\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "seq.compile(loss=\"binary_crossentropy\", optimizer=\"adadelta\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    row = 80\n",
    "    col = 80\n",
    "    noisy_movies = np.zeros((n_samples, n_frames, row, col, 1), dtype=np.float)\n",
    "    shifted_movies = np.zeros((n_samples, n_frames, row, col, 1), dtype=np.float)\n",
    "\n",
    "    for i in range(n_samples):\n",
    "        # Add 3 to 7 moving squares\n",
    "        n = np.random.randint(3, 8)\n",
    "\n",
    "        for j in range(n):\n",
    "            # Initial position\n",
    "            xstart = np.random.randint(20, 60)\n",
    "            ystart = np.random.randint(20, 60)\n",
    "            # Direction of motion\n",
    "            directionx = np.random.randint(0, 3) - 1\n",
    "            directiony = np.random.randint(0, 3) - 1\n",
    "\n",
    "            # Size of the square\n",
    "            w = np.random.randint(2, 4)\n",
    "\n",
    "            for t in range(n_frames):\n",
    "                x_shift = xstart + directionx * t\n",
    "                y_shift = ystart + directiony * t\n",
    "                noisy_movies[\n",
    "                    i, t, x_shift - w : x_shift + w, y_shift - w : y_shift + w, 0\n",
    "                ] += 1\n",
    "\n",
    "                # Make it more robust by adding noise.\n",
    "                # The idea is that if during inference,\n",
    "                # the value of the pixel is not exactly one,\n",
    "                # we need to train the model to be robust and still\n",
    "                # consider it as a pixel belonging to a square.\n",
    "                if np.random.randint(0, 2):\n",
    "                    noise_f = (-1) ** np.random.randint(0, 2)\n",
    "                    noisy_movies[\n",
    "                        i,\n",
    "                        t,\n",
    "                        x_shift - w - 1 : x_shift + w + 1,\n",
    "                        y_shift - w - 1 : y_shift + w + 1,\n",
    "                        0,\n",
    "                    ] += (noise_f * 0.1)\n",
    "\n",
    "                # Shift the ground truth by 1\n",
    "                x_shift = xstart + directionx * (t + 1)\n",
    "                y_shift = ystart + directiony * (t + 1)\n",
    "                shifted_movies[\n",
    "                    i, t, x_shift - w : x_shift + w, y_shift - w : y_shift + w, 0\n",
    "                ] += 1\n",
    "\n",
    "    # Cut to a 40x40 window\n",
    "    noisy_movies = noisy_movies[::, ::, 20:60, 20:60, ::]\n",
    "    shifted_movies = shifted_movies[::, ::, 20:60, 20:60, ::]\n",
    "    noisy_movies[noisy_movies >= 1] = 1\n",
    "    shifted_movies[shifted_movies >= 1] = 1\n",
    "    return noisy_movies, shifted_movies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 1  # In practice, you would need hundreds of epochs.\n",
    "\n",
    "noisy_movies, shifted_movies = generate_movies(n_samples=1200)\n",
    "seq.fit(\n",
    "    noisy_movies[:1000],\n",
    "    shifted_movies[:1000],\n",
    "    batch_size=10,\n",
    "    epochs=epochs,\n",
    "    verbose=2,\n",
    "    validation_split=0.1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_index = 1004\n",
    "test_movie = noisy_movies[movie_index]\n",
    "\n",
    "# Start from first 7 frames\n",
    "track = test_movie[:7, ::, ::, ::]\n",
    "\n",
    "# Predict 16 frames\n",
    "for j in range(16):\n",
    "    new_pos = seq.predict(track[np.newaxis, ::, ::, ::, ::])\n",
    "    new = new_pos[::, -1, ::, ::, ::]\n",
    "    track = np.concatenate((track, new), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lstm autoencoder recreate sequence\n",
    "from numpy import array\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dense\n",
    "from keras.layers import RepeatVector\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.utils import plot_model\n",
    "# define input sequence\n",
    "sequence = array([0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9])\n",
    "# reshape input into [samples, timesteps, features]\n",
    "n_in = len(sequence)\n",
    "sequence = sequence.reshape((1, n_in, 1))\n",
    "# define model\n",
    "model = Sequential()\n",
    "model.add(LSTM(100, activation='relu', input_shape=(n_in,1)))\n",
    "model.add(RepeatVector(n_in))\n",
    "model.add(LSTM(100, activation='relu', return_sequences=True))\n",
    "model.add(TimeDistributed(Dense(1)))\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "# fit model\n",
    "model.fit(sequence, sequence, epochs=300, verbose=0)\n",
    "plot_model(model, show_shapes=True, to_file='reconstruct_lstm_autoencoder.png')\n",
    "# demonstrate recreation\n",
    "yhat = model.predict(sequence, verbose=0)\n",
    "print(yhat[0,:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf2.0-gpu",
   "language": "python",
   "name": "tf2.0-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
