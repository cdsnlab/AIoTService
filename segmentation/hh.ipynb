{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "import packages\n",
    "\"\"\"\n",
    "import os, glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math, random\n",
    "from datetime import datetime\n",
    "import pickle\n",
    "from matplotlib import pyplot as plt\n",
    "import argparse\n",
    "import path, sys, re, time\n",
    "from collections import Counter\n",
    "from scipy.signal import find_peaks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "import custom modules\n",
    "\"\"\"\n",
    "from module_.readText import read_hh\n",
    "from module_.featureExtraction import feature_extraction\n",
    "from module_.changePointDetection import change_point_detection\n",
    "from module_.info.hh import baseline_activities\n",
    "from module_.info.config import feature_name, exclude_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# hh101 (CASAS, 1-resident, serialized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "0. load casas dataset: hh101\n",
    "\"\"\"\n",
    "# with open('dataset/hh/hh101/ann.txt','rb') as f: \n",
    "#     rawdata=f.readlines()\n",
    "# events=read_hh(rawdata)\n",
    "# events=np.array(events)\n",
    "events=np.load(\"./preprocessed/test/ann.npy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Replication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "1. remove all except M and D\n",
    "\"\"\"\n",
    "data_name='hh101'\n",
    "metric='SEP'\n",
    "\n",
    "# EXCLUDE \"IDLE\" and INCLUDE ONLY \"MOTION and DOOR\"\n",
    "events_md=np.array([event for event in events if event[3]!=\"\" and event[0][0] in ['M', 'D']])\n",
    "\n",
    "# MAPPING TO PRE-DEFINED ACTIVITY\n",
    "for i in range(events_md.shape[0]):\n",
    "    events_md[i,3]=baseline_activities[events_md[i,3]]\n",
    "\n",
    "# TRANSITION POINTS\n",
    "trs_md = np.array([i for i in range(len(events_md)) if events_md[i][3]!=events_md[max(i-1,0)][3]])\n",
    "\n",
    "# Sensor List\n",
    "sensor_list = sorted(set(events_md[:,0]))\n",
    "\n",
    "print(\"\"\"\n",
    "    The number of events: {},\n",
    "    The number of transitions: {},\n",
    "    The number of sensors: {}\n",
    "\n",
    "\"\"\".format(len(events_md), len(trs_md), len(sensor_list)))\n",
    "\n",
    "raw_features = np.array(feature_extraction(events_md, data_name, sensor_list))\n",
    "\n",
    "choose_features = []\n",
    "for i in range(raw_features.shape[1]):\n",
    "    if i not in exclude_list['A']:\n",
    "        choose_features.append(raw_features[:,i].reshape(-1,1))\n",
    "choose_features = np.concatenate(choose_features, axis=1)\n",
    "\n",
    "print(raw_features.shape, choose_features.shape)\n",
    "\n",
    "scores = np.array(change_point_detection(choose_features, data_name=data_name, metric=metric))\n",
    "\n",
    "np.save(\"./replication/hh101/scores.npy\", scores)\n",
    "np.save(\"./replication/hh101/transitions.npy\", trs_md)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = np.load(\"./replication/hh101/scores.npy\")\n",
    "transitions = np.load(\"./replication/hh101/transitions.npy\")\n",
    "\n",
    "threshold = 0.1\n",
    "interval = 10\n",
    "# Reject scores lower than threshold\n",
    "# scores[scores<threshold] = 0\n",
    "\n",
    "# Choose peak scores (one score in the range: 2*interval)\n",
    "finalists=[]\n",
    "prevtimestamp = 0\n",
    "for i in range(len(scores)):\n",
    "    # if scores[i]>threshold and float(events_md[i, 2])-prevtimestamp>2*interval:\n",
    "    #     finalists.append(i)\n",
    "    #     prevtimestamp = float(events_md[i, 2])\n",
    "    if scores[i]>threshold:\n",
    "        finalists.append(i)\n",
    "\n",
    "groundtruth = np.array([float(events_md[i, 2]) for i in transitions])\n",
    "\n",
    "# Evaluation\n",
    "truePositive = trueNegative = falsePositive = falseNegative = 0\n",
    "for i in range(len(events_md)):\n",
    "    if i in finalists: # Positive\n",
    "        if i in trs_md: # True\n",
    "            truePositive+=1\n",
    "        else:\n",
    "            timeGap = abs(groundtruth - float(events_md[i, 2]))\n",
    "            if sum(timeGap<interval)!=0:\n",
    "                truePositive+=1\n",
    "            else:\n",
    "                falsePositive+=1\n",
    "    else: # Negative\n",
    "        if i in trs_md: # False\n",
    "            falseNegative+=1\n",
    "        else:\n",
    "            trueNegative+=1\n",
    "\n",
    "print(truePositive/(truePositive+falseNegative))\n",
    "print(falsePositive/(falsePositive+trueNegative))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "episodes, trs, tags = [], [], []\n",
    "previdx=0\n",
    "for i in range(len(trs_md)-1):\n",
    "    left=np.array(events_md[previdx:trs_md[i]])\n",
    "    right=np.array(events_md[trs_md[i]:trs_md[i+1]])\n",
    "    episode=np.concatenate((left, right))\n",
    "    episodes.append(episode)\n",
    "    trs.append(left.shape[0])\n",
    "    pairname=\"{}-{}\".format(left[0][3], right[0][3])\n",
    "    tags.append(pairname)\n",
    "    previdx=trs_md[i]\n",
    "\n",
    "data_name = 'hh101'\n",
    "metric = 'SEP'\n",
    "dataset_folder = './features/{}'.format(data_name)\n",
    "\n",
    "sensor_list = sorted(set(events_md[:,0]))\n",
    "# print(len(sensor_list))\n",
    "\n",
    "if not os.path.exists(dataset_folder):\n",
    "    os.mkdir(dataset_folder)\n",
    "\n",
    "# fig = plt.figure(constrained_layout=True, figsize=(15, 15))\n",
    "\n",
    "for ei, eps in enumerate(episodes):\n",
    "    transition_point = trs[ei]\n",
    "    # sensor_list = sorted(set(eps[:,0]))\n",
    "    features = feature_extraction(eps, data_name, sensor_list)\n",
    "    features = np.array(features)\n",
    "\n",
    "    \n",
    "\n",
    "    # pair_folder = '{}/{}'.format(dataset_folder, tags[ei])\n",
    "    # if not os.path.exists(pair_folder):\n",
    "    #     os.mkdir(pair_folder)\n",
    "    # idx_folder = '{}/{}'.format(pair_folder, ei)\n",
    "    # if not os.path.exists(idx_folder):\n",
    "    #     os.mkdir(idx_folder)\n",
    "\n",
    "    folder_ = \"./features/{}/{}/{}\".format(data_name, tags[i], i)\n",
    "    x_ = range(len(eps))\n",
    "    # fig, ax = plt.subplots(12, 1, sharex=True, constrained_layout=True, figsize=(20, 15))\n",
    "\n",
    "    scores=np.array(change_point_detection(features, data_name=data_name, metric=metric))\n",
    "    scores[scores<0]=0\n",
    "    plt.title(tags[i])\n",
    "    plt.bar(range(len(scores)), scores); plt.axvline(x=transition_point, linestyle=':', color='r')\n",
    "    \n",
    "    break\n",
    "    names = list(feature_name.values())\n",
    "\n",
    "    numbasicfeatures = len(names)-2\n",
    "\n",
    "    for i in range(numbasicfeatures):\n",
    "        ax_ = fig.add_subplot(numbasicfeatures+1, 1, i+1)\n",
    "        ax_.plot(x_, features[:,i], '.-')\n",
    "        ax_.set_ylabel(names[i])\n",
    "        ax_.axvline(transition_point, linestyle=':', color='r')\n",
    "        ax_.set_ylim(0,2)\n",
    "    ax_ = fig.add_subplot(numbasicfeatures+1, 1, numbasicfeatures+1)\n",
    "    ax_.bar(x_, scores, color='g')\n",
    "    ax_.set_ylabel(metric)\n",
    "    ax_.axvline(transition_point, linestyle=':', color='r')\n",
    "    ax_.set_ylim(0,1)\n",
    "    fig.savefig(\"{}/feature_basic.png\".format(idx_folder))\n",
    "    fig.clf()\n",
    "\n",
    "    # # fig, ax = plt.subplots(len(sensor_list), 1, sharex=True, constrained_layout=True, figsize=(15, 15))\n",
    "    # for i in range(12, 12+len(sensor_list)):\n",
    "    #     i_ = i - 12\n",
    "    #     ax_ = fig.add_subplot(len(sensor_list), 2, 2*i_+1)\n",
    "    #     ax_.plot(x_, features[:,i], '.-')\n",
    "    #     ax_.set_ylabel(sensor_list[i_])\n",
    "    #     ax_.axvline(transition_point, linestyle=':', color='r')\n",
    "    #     ax_.set_ylim(-0.2,1.2)\n",
    "    \n",
    "    # # fig.savefig(\"{}/feature_count.png\".format(idx_folder))\n",
    "    # # fig.clf()\n",
    "\n",
    "    # # fig, ax = plt.subplots(len(sensor_list), 1, sharex=True, constrained_layout=True, figsize=(15, 15))\n",
    "    # for i in range(12+len(sensor_list), 12+2*len(sensor_list)):\n",
    "    #     i_ = i-(12+len(sensor_list))\n",
    "    #     ax_ = fig.add_subplot(len(sensor_list), 2, 2*(i_+1))\n",
    "    #     ax_.plot(x_, features[:,i], '.-')\n",
    "    #     ax_.set_ylabel(sensor_list[i_])\n",
    "    #     ax_.axvline(transition_point, linestyle=':', color='r')\n",
    "    #     ax_.set_ylim(-0.2,1.2)\n",
    "    \n",
    "    # fig.savefig(\"{}/feature_sensor.png\".format(idx_folder))\n",
    "    # fig.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"make combinations\n",
    "    1. pick one group type\n",
    "    2. pick an activity stream from the group\n",
    "    3. pick another group type w/o type 1.\n",
    "    4. pick an activity stream from the group\n",
    "\"\"\"\n",
    "\n",
    "data_name='hh101'\n",
    "metric='SEP'\n",
    "\n",
    "for i, eps in enumerate(episodes):\n",
    "\n",
    "    if i%100==0:\n",
    "        print(\"{}/{}\".format(i, len(episodes)))\n",
    "\n",
    "    sensor_list=sorted(set(eps[:,0]))\n",
    "    features=feature_extraction(eps, data_name, sensor_list)\n",
    "    folder_=\"./outputs/{}/{}/{}/{}\".format(data_name, metric, tags[i], i)\n",
    "\n",
    "    if not os.path.exists(folder_):\n",
    "        os.mkdir(folder_)\n",
    "\n",
    "    scores=change_point_detection(features, folder_, data_name=data_name, metric=metric, save=False)\n",
    "\n",
    "###\n",
    "    plt.title(\"{}-{}\".format(tags[i], i))\n",
    "    plt.ylabel('score')\n",
    "    plt.xlabel('event')\n",
    "    plt.ylim(0,2)\n",
    "    plt.bar(range(len(scores)), scores)\n",
    "    # plt.bar(peaks, scores[peaks], color='g')\n",
    "    # plt.bar(negative, scores[negative], color='b')\n",
    "    plt.axhline(y=0.1, linestyle=':', color='r', label='threshold')\n",
    "    plt.axvline(x=trs[i], linestyle=':', color='g', label='transition')\n",
    "    plt.legend()\n",
    "\n",
    "    break\n",
    "    plt.savefig(\"{}/graph.png\".format(folder_))\n",
    "    plt.clf()\n",
    "\n",
    "###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    hh101 Evaluation\n",
    "    - load scores\n",
    "\"\"\"\n",
    "\n",
    "data_name='hh101'\n",
    "metric='RuLSIF'\n",
    "\n",
    "total_counts=np.zeros(4)\n",
    "denom = numer = 0\n",
    "for activity_folder in glob.glob(\"./outputs/{}/{}/*\".format(data_name, metric)):\n",
    "    # one type of pairs\n",
    "    activity_pair=activity_folder.split(\"/\")[-1]\n",
    "    print(activity_pair)\n",
    "    pair_counts=np.zeros(4) # TP, FP, TN, FN\n",
    "\n",
    "    for episode_folder in glob.glob(\"{}/*\".format(activity_folder)):\n",
    "        eps_order=int(episode_folder.split(\"/\")[-1])\n",
    "        denom+=1\n",
    "        eps, point=episodes[eps_order], trs[eps_order]\n",
    "        scores=np.load(\"{}/scores.npy\".format(episode_folder))\n",
    "\n",
    "        # peaks, _ = find_peaks(scores)\n",
    "        positives=[i for i in range(len(scores)) if scores[i]>0.1]\n",
    "        numer+=len(positives)\n",
    "        # positives=[i for i in range(len(scores)) if scores[i]>0.3]\n",
    "        ttimestamp=float(eps[point][2])\n",
    "\n",
    "        for i in range(len(scores)):\n",
    "            if i in positives:\n",
    "                if i==point:\n",
    "                    pair_counts[0]+=1\n",
    "                else:\n",
    "                    timestamp_b=float(eps[i-1][2])\n",
    "                    timestamp_a=float(eps[i][2])\n",
    "                    if abs(ttimestamp-timestamp_b)<10 or abs(ttimestamp-timestamp_a)<10:\n",
    "                        pair_counts[0]+=1\n",
    "                    else:\n",
    "                        pair_counts[1]+=1\n",
    "            else:\n",
    "                if i==point:\n",
    "                    pair_counts[3]+=1\n",
    "                else:\n",
    "                    pair_counts[2]+=1\n",
    "    if pair_counts[0]+pair_counts[3]==0 or pair_counts[1]+pair_counts[2]==0:\n",
    "        continue\n",
    "    TPR_=pair_counts[0]/(pair_counts[0]+pair_counts[3])\n",
    "    FPR_=pair_counts[1]/(pair_counts[1]+pair_counts[2])\n",
    "    print(\"Avg. TPR and FPR: ({}, {})\".format(TPR_, FPR_))\n",
    "\n",
    "    total_counts+=pair_counts\n",
    "\n",
    "TPR=total_counts[0]/(total_counts[0]+total_counts[3])\n",
    "FPR=total_counts[1]/(total_counts[1]+total_counts[2])\n",
    "print(\"Total Avg. TPR and FPR: ({}, {})\".format(TPR, FPR))\n",
    "\n",
    "print(numer/denom)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "metadata": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}