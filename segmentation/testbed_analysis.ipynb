{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "import packages\n",
    "\"\"\"\n",
    "import os, glob\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math, random\n",
    "from datetime import datetime\n",
    "import pickle\n",
    "from matplotlib import pyplot as plt\n",
    "import argparse\n",
    "import path, sys, re, time\n",
    "from collections import Counter\n",
    "from scipy.spatial import distance_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "import custom packages\n",
    "\"\"\"\n",
    "from module_.info.testbed_info import activityfiles_new\n",
    "from module_.info.config import config, feature_name\n",
    "from module_.readText import create_episodes, time_correction\n",
    "from module_.featureExtraction import feature_extraction\n",
    "from module_.changePointDetection import change_point_detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "load raw files\n",
    "\"\"\"\n",
    "dir_=\"dataset/testbed/npy/seminar/MS\"\n",
    "task_dict={i:[np.load(\"{}/{}\".format(dir_, name)) for name in v] for i, v in enumerate(activityfiles_new.values())}\n",
    "initial_dict={i:k[0] for i, k in enumerate(activityfiles_new.keys())}\n",
    "label_dict={k[0]:k for k in activityfiles_new.keys()}\n",
    "\n",
    "episodes, trs, tags = create_episodes(task_dict, initial_dict)\n",
    "episodes=[time_correction(eps, trs[i]) for i, eps in enumerate(episodes)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seminar-TechnicalDiscussion\n",
      "Seminar-Chatting\n",
      "TechnicalDiscussion-GroupStudy\n",
      "Chatting-GroupStudy\n",
      "GroupStudy-TechnicalDiscussion\n",
      "Chatting-TechnicalDiscussion\n",
      "GroupStudy-Seminar\n",
      "Chatting-Seminar\n",
      "TechnicalDiscussion-Seminar\n",
      "GroupStudy-Chatting\n",
      "Seminar-GroupStudy\n",
      "TechnicalDiscussion-Chatting\n"
     ]
    }
   ],
   "source": [
    "dataset='testbed'\n",
    "preprocess='MS'\n",
    "metric='RuLSIF'\n",
    "\n",
    "window_size=30\n",
    "threshold=120\n",
    "\n",
    "bigbucket=[]\n",
    "\n",
    "for pairfolder in glob.glob(\"./outputs/{}/{}/*\".format(dataset, preprocess)):\n",
    "    print(pairfolder.split(\"/\")[-1])\n",
    "    for epsfolder in glob.glob(\"{}/*\".format(pairfolder)):\n",
    "        \n",
    "        epsorder, name_, changeidx = epsfolder.split(\"/\")[-1].split(\"_\")\n",
    "\n",
    "        events=np.load(\"{}/events.npy\".format(epsfolder))\n",
    "        features=np.load(\"{}/features.npy\".format(epsfolder))\n",
    "        scores=np.load(\"{}/{}/scores.npy\".format(epsfolder, metric))\n",
    "\n",
    "        sensor_list=sorted(set(events[:,0]))\n",
    "\n",
    "        file_=open(\"{}/report_wo_c.txt\".format(epsfolder), 'w')\n",
    "        file_.write('{} Transition at {}\\n'.format(name_, changeidx))\n",
    "        file_.write('{} {}\\n'.format(sensor_list, len(sensor_list)))\n",
    "\n",
    "        timestamps=list(events[:,2].astype(float))\n",
    "        transitionTimestamp=timestamps[int(changeidx)]\n",
    "\n",
    "        # # find index\n",
    "        timeA=[t for t in timestamps if t-timestamps[0]<threshold][-1]\n",
    "        timeB=[t for t in timestamps if transitionTimestamp-t<threshold][0]\n",
    "        timeC=[t for t in timestamps if t-transitionTimestamp<threshold][-1]\n",
    "        timeD=[t for t in timestamps if timestamps[-1]-t<threshold][0]\n",
    "\n",
    "        idxA=timestamps.index(timeA)\n",
    "        idxB=timestamps.index(timeB)\n",
    "        idxC=timestamps.index(timeC)\n",
    "        idxD=timestamps.index(timeD)\n",
    "\n",
    "        # # event\n",
    "        eventA=events[:idxA+1]\n",
    "        eventB=events[idxB-window_size+1:int(changeidx)]\n",
    "        eventC=events[int(changeidx)-window_size+1:idxC+1]\n",
    "        eventD=events[idxD-window_size+1:]\n",
    "\n",
    "        eventNA=events[max(0, idxA-window_size):idxB]\n",
    "        eventNB=events[max(0, idxC-window_size):idxD]\n",
    "\n",
    "        event_file=open(\"{}/event.txt\".format(epsfolder), 'w')\n",
    "\n",
    "        event_file.write(\"First Start\\n\")\n",
    "        for i in range(len(eventA)):\n",
    "            event_file.write(\"{}\\n\".format(eventA[i]))\n",
    "        event_file.write(\"First End\\n\")\n",
    "        for i in range(len(eventB)):\n",
    "            if i==window_size-1:\n",
    "                event_file.write(\"{} *\\n\".format(eventB[i]))\n",
    "            else:\n",
    "                event_file.write(\"{}\\n\".format(eventB[i]))\n",
    "        event_file.write(\"Second Start\\n\")\n",
    "        for i in range(len(eventC)):\n",
    "            if i==window_size-1:\n",
    "                event_file.write(\"{} *\\n\".format(eventC[i]))\n",
    "            else:\n",
    "                event_file.write(\"{}\\n\".format(eventC[i]))\n",
    "        event_file.write(\"Second End\\n\")\n",
    "        for i in range(len(eventD)):\n",
    "            if i==window_size-1:\n",
    "                event_file.write(\"{} *\\n\".format(eventD[i]))\n",
    "            else:\n",
    "                event_file.write(\"{}\\n\".format(eventD[i]))\n",
    "\n",
    "        event_file.write(\"First Middle\\n\")\n",
    "        for i in range(len(eventNA)):\n",
    "            if i==window_size-1:\n",
    "                event_file.write(\"{} *\\n\".format(eventNA[i]))\n",
    "            else:\n",
    "                event_file.write(\"{}\\n\".format(eventNA[i]))\n",
    "        event_file.write(\"Second Middle\\n\")\n",
    "        for i in range(len(eventNB)):\n",
    "            if i==window_size-1:\n",
    "                event_file.write(\"{} *\\n\".format(eventNB[i]))\n",
    "            else:\n",
    "                event_file.write(\"{}\\n\".format(eventNB[i]))\n",
    "\n",
    "        event_file.close()\n",
    "\n",
    "        # # feature\n",
    "        # featureA=features[:idxA+1]\n",
    "        # featureB=features[idxB:int(changeidx)]\n",
    "        # featureC=features[int(changeidx):idxC+1]\n",
    "        # featureD=features[idxD:]\n",
    "\n",
    "        # # score\n",
    "        # scoreA=scores[:idxA+1]\n",
    "        # scoreB=scores[idxB:int(changeidx)]\n",
    "        # scoreC=scores[int(changeidx):idxC+1]\n",
    "        # scoreD=scores[idxD:]\n",
    "\n",
    "        # lambdas=np.load(\"{}/{}/lambdas.npy\".format(epsfolder, metric))\n",
    "        # sigmas=np.load(\"{}/{}/sigmas.npy\".format(epsfolder, metric))\n",
    "        # thetas=np.load(\"{}/{}/thetas.npy\".format(epsfolder, metric))\n",
    "\n",
    "        # descending=sorted([(score, idx) for idx, score in enumerate(scores)], reverse=True)\n",
    "\n",
    "        # for i in range(100):\n",
    "        #     order=i\n",
    "        #     score__, eidx=descending[order]\n",
    "        #     # print(\"{} Score at {}\".format(score__, eidx))\n",
    "        #     file_.write(\"{} Score at {}\\n\".format(score__, eidx))\n",
    "            \n",
    "        #     e_=events[max(0, eidx-30+1):eidx+3]\n",
    "        #     # file_.write(\"{}\\n{}\\n{}\\n\".format(events[i], events[min(len(events)-1, i+1)], events[min(len(events)-1, i+2)]))\n",
    "        #     file_.write(\"{}\\n\".format(e_))\n",
    "            \n",
    "        #     # f_=features[eidx:eidx+3]\n",
    "\n",
    "        #     f1, f2, f3=features[eidx:eidx+3]\n",
    "            \n",
    "        #     f1f2=np.array(f1)-np.array(f2)\n",
    "\n",
    "        #     f12l=[]\n",
    "        #     L2_12=sum(np.square(f1f2))\n",
    "        #     for i in range(len(f1f2)):\n",
    "        #         if f1f2[i]**2/L2_12>0.1:\n",
    "        #             f12l.append((i, round(f1f2[i]**2/L2_12, 2), round(f1f2[i]**2, 6)))\n",
    "\n",
    "        #     importances_f1f2=sorted(f12l, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "        #     f1f3=np.array(f1)-np.array(f3)\n",
    "\n",
    "        #     f13l=[]\n",
    "        #     L2_13=sum(np.square(f1f3))\n",
    "        #     for i in range(len(f1f3)):\n",
    "        #         if f1f3[i]**2/L2_13>0.1:\n",
    "        #             f13l.append((i, round(f1f3[i]**2/L2_13, 2), round(f1f3[i]**2, 6)))\n",
    "\n",
    "        #     importances_f1f3=sorted(f13l, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "        #     f2f3=np.array(f2)-np.array(f3)\n",
    "        #     f23l=[]\n",
    "\n",
    "        #     L2_23=sum(np.square(f2f3))\n",
    "        #     for i in range(len(f2f3)):\n",
    "        #         if f2f3[i]**2/L2_23>0.1:\n",
    "        #             f23l.append((i, round(f2f3[i]**2/L2_23, 2), round(f2f3[i]**2, 6)))\n",
    "\n",
    "        #     importances_f2f3=sorted(f23l, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "        #     # print(\"1-2\", L2_12)\n",
    "        #     # print(\"1-3\",L2_13)\n",
    "        #     # print(\"2-3\", L2_23)\n",
    "        #     file_.write(\"1-2 {}\\n\".format(L2_12))\n",
    "        #     file_.write(\"1-3 {}\\n\".format(L2_13))\n",
    "        #     file_.write(\"2-3 {}\\n\".format(L2_23))\n",
    "\n",
    "        #     # print(\"1-2 / 2-3\", L2_12/L2_23)\n",
    "        #     # print(\"1-3 / 1-2\", L2_13/L2_12)\n",
    "        #     file_.write(\"1-2 / 2-3 {}\\n\".format(L2_12/L2_23))\n",
    "        #     file_.write(\"1-3 / 1-2 {}\\n\".format(L2_13/L2_12))\n",
    "\n",
    "        #     file_.write(\"f1 and f2 {}\\n\".format(importances_f1f2))\n",
    "        #     file_.write(\"f1 and f3 {}\\n\".format(importances_f1f3))\n",
    "        #     file_.write(\"f2 and f3 {}\\n\".format(importances_f2f3))\n",
    "        #     # print(\"f1 and f2\", importances_f1f2)\n",
    "        #     # print(\"f1 and f3\", importances_f1f3)\n",
    "        #     # print(\"f2 and f3\", importances_f2f3)\n",
    "\n",
    "        # # print(e_, len(e_))\n",
    "        # file_.close()\n",
    "\n",
    "        # for index_ in range(len(events)):\n",
    "\n",
    "        #     score=scores[index_]\n",
    "        #     theta=thetas[index_]\n",
    "        #     lambda_, sigma_ = lambdas[index_], sigmas[index_,0]\n",
    "        #     # print(score, lambda_, sigma_)\n",
    "        #     # print(theta)\n",
    "\n",
    "        #     before=np.array([features[index_], features[min(index_+1, len(events)-1)]])\n",
    "        #     after=np.array([features[min(index_+1, len(events)-1)], features[min(index_+2, len(events)-1)]])\n",
    "\n",
    "        #     L2_test=np.square(distance_matrix(before, before))\n",
    "        #     L2_train=np.square(distance_matrix(before, after))\n",
    "\n",
    "        #     phi_test=np.exp(-L2_test/(2*(sigma_**2)))\n",
    "        #     phi_train=np.exp(-L2_train/(2*(sigma_**2)))\n",
    "\n",
    "        #     # print(phi_test)\n",
    "        #     # print(phi_train)\n",
    "\n",
    "        #     g_x=phi_test.T@theta\n",
    "        #     g_y=phi_train.T@theta\n",
    "\n",
    "        #     # print(g_x)\n",
    "        #     # print(g_y)\n",
    "\n",
    "        #     # for alpha in range(0, 10, 1):\n",
    "        #     #     alpha=float(alpha/10.)\n",
    "        #     #     estimated_score=-0.5-alpha*0.5*np.square(g_x).mean()-(1-alpha)*0.5*np.square(g_y).mean()+g_x.mean()\n",
    "        #     #     print(estimated_score)\n",
    "        #     bucket=[]\n",
    "        #     bucket.append(score)\n",
    "        #     bucket+=list(g_x.ravel())\n",
    "        #     bucket+=list(g_y.ravel())\n",
    "        #     bucket+=list(theta.ravel())\n",
    "        #     bucket+=[lambda_, sigma_]\n",
    "        #     bucket+=list(L2_test.ravel())\n",
    "        #     bucket+=list(L2_train.ravel())\n",
    "        #     bucket+=list(phi_test.ravel())\n",
    "        #     bucket+=list(phi_train.ravel())\n",
    "\n",
    "\n",
    "        #     bigbucket.append(bucket)\n",
    "\n",
    "# np.savetxt(\"analysis.csv\", bigbucket, delimiter=',')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": ""
  },
  "metadata": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}